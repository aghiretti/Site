---
title: ""
author: "Alessandro Ghiretti"
date: "1/2/2021"
output: html_document
draft: True
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Ad clicks predictions


```{r}
library(paradox)
library(dplyr)
library(tidyr)
library(ggplot2)
library(GGally)
library(reshape2)
library(mlr3verse)
library(mlr3extralearners)
library(mlr3pipelines)
library(cattonum)
```


```{r}
setwd("C:\\Users\\USR02193\\OneDrive - Chiesi Farmaceutici S.p.A\\Desktop\\Blog\\Data\\PPC\\")

# load train and test data
train <- read.csv("train.csv")
test <- read.csv("test.csv")
```


```{r}
# number of advertisers
n_adv <- length(unique(train$advertiser_id))
n_usr <- length(unique(train$user_id))
n_kyw <- length(unique(train$keyword_id))
```


We compute the metrics aggreated at three different levels

```{r}
######################## METRICS ######################
# 1) ad metrics
ad_metrics <- train %>% group_by(ad_id) %>% 
  summarise(
    n_clicks = sum(click),
    tot_clicks = length(click),
    prc_clicks = (sum(click)/length(click))*100) %>% 
  arrange(desc(n_clicks))
  
# 2) advertiser metrics
adv_metrics <- train %>% group_by(advertiser_id) %>% 
  summarise(
    n_clicks = sum(click),
    tot_clicks = length(click),
    prc_clicks = (sum(click)/length(click))*100) %>% 
  arrange(desc(n_clicks))

# 3) keyword metrics
adv_keyword <- train %>% group_by(keyword_id) %>% 
  summarise(
    n_clicks = sum(click),
    tot_clicks = length(click),
    prc_clicks = (sum(click)/length(click))*100) %>% 
  arrange(desc(n_clicks))
```


## Data inspection

```{r}
train %>% select(click) %>% melt() %>% ggplot()+
geom_bar(aes(x = value, fill = factor(value)), alpha = 0.7) + theme_minimal()
```
The number of clicked ad is equal to

```{r}
sum(train$click)
```
```{r}
sum(!train$click)
```
with a class ratio of

```{r}
(sum(train$click)/nrow(train))*100
```


The dataset is highly imbalanced and rebalancing strategy must be implemented.
In this case a cost sensitive learning approach could represent the best option
but unfortunately we do not have any informatio that can be used to assign
costs to the two different classes. We opt for a resampling strategy.
Given the high imbalance score oversampling would probably introduce much noise
by producing many replicates of the records which belongs to class 1. 
Since the dataset is not so small in this case undersampling, which reduces the observations
in the 0 class might be a better choice. We decide to proceed further using
both SMOTE and undersampling.
We proceed further inspecting the feature variables

```{r}
print(apply(train,2, function(x) length(unique(x))))
```

All the features that can be used to predict the click are factors and many of them
can assume a huge number of values. The one-hot encoding could be adopted to 
code the depth, position and impression but appear unreasonable for the other variables
since the dimension of the feature space would explode producing course of dimension.
In order to overcome this difficulty we decode categorical variable witht he frequency
encoding.




## Model construction

In practical applications it is common that different steps are involved in
constructing a machine learning model such as data pre processing, feature reduction
model validation and so on.
In order to standardize all these procedure ML pipelines adopted.
In MLR3 pipelines are represented by graphs, where each node correspond to 
an operation which is performed on the data and the lines define the input/output
relationships between the operations.
A pipeline is generally constructed by first defining the single transformations
and/or elements and subsequently by merging all the elements in the pipeline graph.
Despite in this case we are not going to put in production the model or repeat
the workflow on new data we adopt this pipeline approach as it is closer to
what happens in reality.

In the first part we will define all the elements that will be used in building
the graph, while in the second part we will compose the workflow pipeline.

### Task and model definition
The first ingredient that we need is to convert our training data to a classification task.

```{r}
# define the classification task
train$click <- factor(train$click)
task <- TaskClassif$new(id = "ppc", train, target = "click")
```

Next, we define the learners that we will use for predicting the click of 
a customer. We will use kernel support vector machine, kernel nearest neighbors and extreme
gradient  boosting.

```{r, echo = TRUE}
# define learners and ids
learners <- c("classif.ksvm", "classif.kknn", "classif.xgboost")
# obtain the selected learners
learners <- lapply(learners,lrn)
```

### Imbalance data resampling
In order to balance the data we will adopt two different sampling strategies 
We will consider under and over sampling. Both methods
work by randomly sampling observations from the two classes. Undersampling
balance the dataset by subsampling units from the majority class, while oversampling
augment the minority class by randomly sampling observations.

```{r, echo = TRUE}
# define balancing pipeline
po_under <- po("classbalancing", id = "undersample", adjust = "minor",
  reference = "minor", shuffle = FALSE, ratio = 1 / 6) 

po_over <- po("classbalancing", id = "oversample", adjust = "major",
  reference  = "major", shuffle = FALSE, ratio = 6)
```

### Hyperparameters search space
We now define the hyperparameters and the hyperparameter space that will be subsequently 
optimized. A full optimization of the hyperparameter space would be too expensive
from a computational perspective therefore here we will restrict ourselves to a



### Hyperparameters tuning strategy
The fourth step is to define how to tune the hyperparameters and how to select
the best performing model. Two paths can be generally follow. The first consists
in splitting splitting the data 
into three sets:

 * **Training**. Which is used to train different models and compare their
  performances.
  
 * **Validation**. Which is used to tune the hyperparameters of a model.
 
 * **Test**. Which is used to obtain an unbiased estimate of the classification error.
 
On one hand this first approach requires  that a huge amount of data is available and
on the other hand, that it is possible to produce three homogeneous sets.
In our case since it might be difficult to identify a rule that can be used to split
the data into two sets with similar characteristics we adopt a nested cross validation approach.
In order to score the different models we will monitor four different metrics

* Classification Error
* Specitivity
* Sensitivity
* AUC

Classification Error and AUC represent two standard metric which are used to 
evaluate the performances of a classifier when the data are balanced. However,
when data are unbalanced the Specificity and Sensitivity represent better metrics
as they allow to see the goodness that the classifier can achieve on the two 
different classes. Finally as a searching strategy for hyperparameters
optimization we adopt a random search method.


```{r, echo = TRUE}
# define the sampling strategy
inner_resampler <- rsmp("cv", folds = 10L)

# define the outer resampler
outer_resampler <- rsmp("cv", folds = 5L)

# define the tuner
tuner <- tnr("random_search")
```

Since nested cross validation is obtained with two nested CV loops we first
have to encapsulate all the information in the learner which now encompass the
nested CV steps and subsequently apply the outer sampling strategy.

```{r}
# create search space 


# create the autotuner for each learner
auto_tuner <- lapply(learners, function(x) AutoTuner$new(
  learner = x,
  resampling = resampling,
  measure = measure,
  search_space = search_space,
  terminator = terminator,
  tuner = tuner
))
```

Having defined all the ingredients it is now to time to put them together in 
the workflow graph. Since we are going to compare three different models
we are going to build three different graphs

```{r}
# define the function for the graph
make_graph <- function(learner){
  po("colapply", applicator = function(x) catto_freq(x)) %>>%
  po("branch",id = "branch_imbalance_sampling", options = c("under", "over")) %>>%
  gunion(list(po_under, po_over)) %>>%
  po("unbranch", id = "unbranch_imbalance_sampling") %>>%
  lrn(learner)}
  
graphs <- lapply(auto_tuner)
```


### Model fitting
Hving created the graph it is now time to