# plot model residuals
p1 <- mod.res %>% ggplot() +
geom_point(aes(x = Time, y = .resids), color = "royalblue", alpha = 0.4) +
theme_minimal() +
ylab("residuals") + ggtitle("Fitted residuals")
ggplotly(p1)
?MARSS
# dlm with autoregressive states
dlr <- MARSS(response, inits = inits.list, model = mod.list.ar,
control = cntl.list, silent = TRUE, method="BFGS", inits=kemfit)
# dlm with autoregressive states
dlr <- MARSS(response, model = mod.list.ar,
control = cntl.list, silent = TRUE, method="BFGS", inits=kemfit)
dlr <- MARSS(response, method="BFGS", inits=kemfit, model = mod.list,
control = cntl.list, silent = TRUE)
# list of model matrices & vectors
mod.list <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R)
dlr <- MARSS(response, method="BFGS", inits=kemfit, model = mod.list,
control = cntl.list, silent = TRUE)
dlr <- MARSS(response, method="BFGS", inits=dlr.em, model = mod.list,
control = cntl.list, silent = TRUE)
dlr <- MARSS(response, method="BFGS", inits=dlr, model = mod.list,
control = cntl.list, silent = TRUE)
dlr <- MARSS(response, method="BFGS", inits=dlr, model = mod.list, silent = TRUE)
autoplot(dlr)
dlr
# dlm with autoregressive states
dlr.em <- MARSS(response, inits = inits.list, model = mod.list.ar,
control = cntl.list, silent = TRUE)
# dlm with autoregressive states
dlr.em <- MARSS(response, inits = inits.list, model = mod.list.ar,
control = cntl.list, silent = TRUE)
dlr <- MARSS(response, method="BFGS", inits=dlr, model = mod.list, silent = TRUE)
# obtain smoothed states
s<- tsSmooth(dlr)
colnames(s) <- c("state", "Year", "val","se")
state.names <- c("intercept",colnames(advertising)[2:m])
s$state <- rep(state.names,each = 200)
p1 <- s %>% ggplot() +
geom_line(aes(x = Year, y = val,color = state),
size = 0.4, alpha = 0.7) +
theme_minimal()+
ggtitle("States") +
facet_wrap(~ state,ncol = 4, scales = 'free')
ggplotly(p1)
logLik(dlr)
logLik(dlr.em)
AIC(dlr)
AIC(dlr.em)
BIC(dlr.em)
BIC(dlr)
?AIC
?logLik.marssMLE
MARSSaic(dlr)
dlr.em
library(broom)
glance(s)
tidy(S)
tidy(s)
tidy(dlr)
glance(dlr)
glance(dlr.em)
AIC(dlr)
AIC(dlr.em)
# STATE EQUATION
# specify the B matrix
B <- diag(m)
# specify B matrix for AR1 states
diag(B)[2:13] <- paste("b",2:13, sep = "")
# specify remaining matrices
U <- matrix(0, nrow = m, ncol = 1)
Q <- "unconstrained"
# OBSERVATION EQUATION
Z <- array(NA, c(1, m, TT)) # NxMxT; empty for now
Z[1, 1, ] <- rep(1, TT) # Nx1; 1's for intercept
Z[1, 2:13, ] <- covariates # Nx1; regr variable
A <- matrix(0) # 1x1; scalar = 0
R <- matrix("r") # 1x1; scalar = r
# initialize thate vector
inits.list <- list(x0 = matrix(0, nrow = m))
# set parameters for optimizer
cntl.list <- list(minit = 200, maxit = 20000, allow.degen = FALSE)
# list of model matrices & vectors
mod.list <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R)
# STATE EQUATION
# specify the B matrix
B <- diag(m)
# specify B matrix for AR1 states
diag(B)[2:13] <- paste("b",2:13, sep = "")
# specify remaining matrices
U <- matrix(0, nrow = m, ncol = 1)
Q <- "unconstrained"
# OBSERVATION EQUATION
Z <- array(NA, c(1, m, TT)) # NxMxT; empty for now
Z[1, 1, ] <- rep(1, TT) # Nx1; 1's for intercept
Z[1, 2:13, ] <- covariates # Nx1; regr variable
A <- matrix(0) # 1x1; scalar = 0
R <- matrix("r") # 1x1; scalar = r
# initialize thate vector
inits.list <- list(x0 = matrix(0, nrow = m))
# set parameters for optimizer
cntl.list <- list(minit = 200, maxit = 20000, allow.degen = FALSE)
# list of model matrices & vectors
mod.list <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R)
# dlm with autoregressive states
dlr <- MARSS(response, inits = inits.list, model = mod.list.ar,
control = cntl.list, silent = TRUE)
dlr
# set parameters for optimizer
cntl.list <- list(minit = 200, maxit = 20000)
# list of model matrices & vectors
mod.list <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R)
# list of model matrices & vectors
mod.list <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R)
And finally we fit the model.
```{r, echo = FALSE}
# dlm with autoregressive states
dlr <- MARSS(response, inits = inits.list, model = mod.list.ar,
control = cntl.list, silent = TRUE)
dlr
dlr$par
dlr$control
dlr$convergence
# obtain smoothed states
s<- tsSmooth(dlr)
colnames(s) <- c("state", "Year", "val","se")
state.names <- c("intercept",colnames(advertising)[2:m])
s$state <- rep(state.names,each = 200)
p1 <- s %>% ggplot() +
geom_line(aes(x = Year, y = val,color = state),
size = 0.4, alpha = 0.7) +
theme_minimal()+
ggtitle("States") +
facet_wrap(~ state,ncol = 4, scales = 'free')
ggplotly(p1)
t <- advertising$Time
yhat <- forecast.marssMLE(dlr, type = "ytt1")$pred
colnames(yhat)[2:9] <- c("Time","sales", "predicted_sales", "se", "lo", "hi",
"LO", "HI")
yhat <- yhat[1:200,]
yhat$Time <- t
p1 <- yhat %>% ggplot() +
geom_point(aes(x = Time, y = sales), color = "royalblue",
size = 1, alpha = 0.7) +
geom_line(aes(x = Time, y = predicted_sales), color = "orange",
size = 0.8, alpha = 0.7) +
geom_point(aes(x = Time, y = predicted_sales), color = "orange",
size = 1, alpha = 0.7) +
theme_minimal()+  ggtitle("States")
ggplotly(p1)
# extract model residuals
mod.res = autoplot(dlr, plot.type = "model.resids")$data
mod.res$Time = t
# extract state residuals
state.res <- autoplot(dlr, plot.type = "state.resids")$data
# plot model residuals
p1 <- mod.res %>% ggplot() +
geom_point(aes(x = Time, y = .resids), color = "royalblue", alpha = 0.4) +
theme_minimal() +
ylab("residuals") + ggtitle("Fitted residuals")
ggplotly(p1)
p1 <- state.res %>% ggplot() +
geom_point(aes(x = Year, y = .resids, group = .rownames), color = "royalblue", alpha = 0.4) +
geom_ribbon(aes(x = Year, ymin = - 1.96*.sigma,
ymax = +1.96*.sigma), alpha = 0.3) +
facet_wrap(~.rownames, ncol = 2) +
theme_minimal() +
ylab("residuals") + ggtitle("State residuals")
ggplotly(p1)
state.res$Time <- rep(t,13)
13*200
state.res$Time <- rep(t,each = 13)
state.res$Time <- rep(t[-1],each = 13)
t
state.res
state.res$Time <- rep(t[2:length(t)],each = 13)
state.res$Time <- rep(t[2:length(t)], 13)
head(State.res)
head(state.res)
# plot model residuals
p1 <- mod.res %>% ggplot() +
geom_point(aes(x = Time, y = .resids), color = "royalblue", alpha = 0.4) +
theme_minimal() +
ylab("residuals") + ggtitle("Fitted residuals")
ggplotly(p1)
p1 <- state.res %>% ggplot() +
geom_point(aes(x = Year, y = .resids), color = "royalblue", alpha = 0.4) +
geom_ribbon(aes(x = Year, ymin = - 1.96*.sigma,
ymax = +1.96*.sigma), alpha = 0.3) +
theme_minimal() +
ylab("residuals") + ggtitle("State residuals")
ggplotly(p1)
p1 <- state.res %>% ggplot() +
geom_point(aes(x = Time, y = .resids), color = "royalblue", alpha = 0.4) +
geom_ribbon(aes(x = Time, ymin = - 1.96*.sigma,
ymax = +1.96*.sigma), alpha = 0.3) +
theme_minimal() +
ylab("residuals") + ggtitle("State residuals")
ggplotly(p1)
p1 <- state.res %>% ggplot() +
geom_point(aes(x = Time, y = .resids), color = "royalblue", alpha = 0.4) +
geom_ribbon(aes(x = Time, ymin = - 1.96*.sigma,
ymax = +1.96*.sigma), alpha = 0.3) +
facet_wrap(~.rownames, ncol = 2) +
theme_minimal() +
ylab("residuals") + ggtitle("State residuals")
ggplotly(p1)
p1 <- state.res %>% ggplot() +
geom_point(aes(x = Time, y = .resids), color = "royalblue", alpha = 0.4) +
geom_ribbon(aes(x = Time, ymin = - 1.96*.sigma,
ymax = +1.96*.sigma), alpha = 0.3) +
facet_wrap(~.rownames, ncol =4) +
theme_minimal() +
ylab("residuals") + ggtitle("State residuals")
ggplotly(p1)
p1 <- state.res %>% ggplot() +
geom_point(aes(x = Time, y = .resids), color = "royalblue", alpha = 0.4) +
geom_ribbon(aes(x = Time, ymin = - 1.96*.sigma,
ymax = +1.96*.sigma), alpha = 0.3) +
facet_wrap(~.rownames, ncol =4, scales = "free") +
theme_minimal() +
ylab("residuals") + ggtitle("State residuals")
ggplotly(p1)
# plot model residuals
p1 <- mod.res %>% ggplot() +
geom_point(aes(x = Time, y = .resids), color = "royalblue", alpha = 0.4) +
geom_ribbon(aes(x = Time, ymin = - 1.96*.sigma,
ymax = +1.96*.sigma), alpha = 0.3) +
theme_minimal() +
ylab("residuals") + ggtitle("Fitted residuals")
ggplotly(p1)
# plot model residuals
p1 <- mod.res %>% ggplot() +
geom_point(aes(x = Time, y = .resids), color = "royalblue", alpha = 0.4) +
ylab("residuals") + ggtitle("Fitted residuals")
p1 <- mod.res %>% ggplot() +
geom_qq(aes(sample = .resids, group = country), color = "royalblue", alpha = 0.4) +
stat_qq_line(aes(sample = .resids, group = country), color = "red", alpha = 0.4) +
theme_minimal() +
ggtitle("State residuals")
ggplotly(p1)
p1 <- mod.res %>% ggplot() +
geom_qq(aes(sample = .resids), color = "royalblue", alpha = 0.4) +
stat_qq_line(aes(sample = .resids), color = "red", alpha = 0.4) +
theme_minimal() +
ggtitle("State residuals")
ggplotly(p1)
p1 <- state.res %>% ggplot() +
geom_qq(aes(sample = .resids, group = .rownames), color = "royalblue", alpha = 0.4) +
stat_qq_line(aes(sample = .resids, group = .rownames), color = "red", alpha = 0.4) +
facet_wrap(~.rownames, ncol = 4) +
theme_minimal() +
ggtitle("State residuals")
ggplotly(p1)
# extract model residuals
mod.res = autoplot(dlr, plot.type = "model.resids")$data
mod.res$Time <- t
# extract state residuals
state.res <- autoplot(dlr, plot.type = "state.resids")$data
state.res$Time <- rep(t[2:length(t)], 13)
# plot model residuals
p1 <- mod.res %>% ggplot() +
geom_point(aes(x = Time, y = .resids), color = "royalblue", alpha = 0.4) +
ylab("residuals") + ggtitle("Fitted residuals")
ggplotly(p1)
# plot model residuals
p1 <- mod.res %>% ggplot() +
geom_point(aes(x = Time, y = .resids), color = "royalblue", alpha = 0.4) +
ylab("residuals") + ggtitle("Fitted residuals") +
theme_minimal()
ggplotly(p1)
# plot model residuals
p1 <- mod.res %>% ggplot() +
geom_point(aes(x = Time, y = .resids), color = "royalblue", alpha = 0.4) +
ylab("residuals") + ggtitle("Fitted residuals") +
theme_minimal()
ggplotly(p1)
install.packages("mlr3viz")
rm(list = ls())
library(dplyr)
library(tidyr)
library(ggplot2)
library(GGally)
library(mlr3verse)
library(mlr3extralearners)
setwd("C:\\Users\\USR02193\\OneDrive - Chiesi Farmaceutici S.p.A\\Desktop\\Blog\\Data\\PPC\\")
# load train and test data
train <- read.csv("train.csv")
test <- read.csv("test.csv")
# number of advertisers
n_adv <- length(unique(train$advertiser_id))
n_usr <- length(unique(train$user_id))
n_kyw <- length(unique(train$keyword_id))
######################## METRICS ######################
# 1) ad metrics
ad_metrics <- train %>% group_by(ad_id) %>%
summarise(
n_clicks = sum(click),
tot_clicks = length(click),
prc_clicks = (sum(click)/length(click))*100) %>%
arrange(desc(n_clicks))
# 2) advertiser metrics
adv_metrics <- train %>% group_by(advertiser_id) %>%
summarise(
n_clicks = sum(click),
tot_clicks = length(click),
prc_clicks = (sum(click)/length(click))*100) %>%
arrange(desc(n_clicks))
# 3) keyword metrics
adv_keyword <- train %>% group_by(keyword_id) %>%
summarise(
n_clicks = sum(click),
tot_clicks = length(click),
prc_clicks = (sum(click)/length(click))*100) %>%
arrange(desc(n_clicks))
#####################################################
library(reshpae2)
knitr::opts_chunk$set(echo = TRUE)
train %>% select(click, keyword_id) %>% melt()
library(reshape2)
```{r}
```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(GGally)
library(reshape2)
library(mlr3verse)
library(mlr3extralearners)
train %>% select(click, keyword_id) %>% melt()
train %>% select(click) %>% melt()
train %>% select(click, keyword_id) %>% melt()
train %>% select(click, keyword_id)
train %>% select(click, keyword_id) %>%
ggplot()+
geom_bar(aes(x = kewiword_id, fill = interaction(click,keyword_id)),
stat = "count") + theme_minimal()
train %>% select(click, keyword_id) %>%
ggplot()+
geom_bar(aes(x = keyword_id, fill = interaction(click,keyword_id)),
stat = "count") + theme_minimal()
train %>% select(click) %>% melt() %>% ggplot()+
geom_bar(aes(x = value, fill = factor(value)), alpha = 0.7) + theme_minimal()
(sum(train$click)/nrow(train))+100
(sum(train$click)/nrow(train))*100
sum(train$click)
sum(|train$click)
sum(!train$click)
# define the classification task
train$click <- factor(train$click)
task <- TaskClassif$new(id = "ppc",backend = train,target = "click")
table(task$true())
table(task$true)
table(task$truth())
table(task$truth())
task$truth()
sum(task$truth())/task$nrow
mm = table(task$truth())
mm
mm[0]
mm[1]
mm[1]/mm[2]
t <- table(task$truth())
t
t[2]/t[1]
t[2]/t[1]*100
t[2]
t[2]/task$nrow
(t[2]/task$nrow)*100
?po
knn <- lrn("classif.knn")
knn <- lrn("classif.kknn")
install_learners("classif.knn")
install_learners("classif.kknn")
knn <- lrn("classif.kknn")
knn$param_set
?tnr
?rsmp
knitr::opts_chunk$set(echo = TRUE)
train %>% n_distinct()
```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(GGally)
library(reshape2)
library(mlr3verse)
library(mlr3extralearners)
setwd("C:\\Users\\USR02193\\OneDrive - Chiesi Farmaceutici S.p.A\\Desktop\\Blog\\Data\\PPC\\")
# load train and test data
train <- read.csv("train.csv")
test <- read.csv("test.csv")
# number of advertisers
n_adv <- length(unique(train$advertiser_id))
n_usr <- length(unique(train$user_id))
n_kyw <- length(unique(train$keyword_id))
train %>% n_distinct()
apply(train,2,function(x) length(unique(x)))
data.table(apply(train,2,function(x) length(unique(x))))
as.data.table(apply(train,2,function(x) length(unique(x))))
data.frame(apply(train,2,function(x) length(unique(x))))
print(apply(train,2,function(x) length(unique(x))))
count(train$click)
# obtain the top 10 values for every variable
freq.item <- list()
for(i in colnames(train)){
freq.item[[i]] <- train %>%
group_by(i) %>%
summarise(n_item = n()) %>%
arrange(desc(n_item)) %>%
top_n(6)
}
i
freq.item[[i]] <- train %>%
group_by_(i) %>%
summarise(n_item = n()) %>%
arrange(desc(n_item)) %>%
top_n(6)
freq.item
# obtain the top 10 values for every variable
freq.item <- list()
for(i in colnames(train)){
freq.item[[i]] <- train %>%
group_by_(i) %>%
summarise(n_item = n()) %>%
arrange(desc(n_item)) %>%
top_n(6)
}
freq.item
# obtain the top 10 values for every variable
freq.item <- list()
for(i in colnames(train)){
freq.item[[i]] <- train %>%
group_by_(i) %>%
summarise(n_item = n()) %>%
arrange(desc(n_item)) %>%
top_n(10)
}
freq.item
# obtain the top 10 values for every variable
freq.item <- list()
for(i in colnames(train)){
freq.item[[i]] <- train %>%
group_by_(i) %>%
summarise(n_item = n()) %>%
arrange(desc(n_item)) %>%
top_n(20)
}
freq.item
xgb <- lrn("classif.xgboost")
# define the learners that we will compare
ksvm <- lrn("classif.ksvm") # kernel support vector machine
# define the learners that we will compare
ksvm <- lrn("classif.ksvm") # kernel support vector machine
knn <- lrn("classif.kknn") # neares neighoburs
# define the learners that we will compare
ksvm <- lrn("classif.ksvm") # kernel support vector machine
knn <- lrn("classif.kknn") # neares neighoburs
xgb <- lrn("classif.xgboost") # gradient boosting machine
learners <- c("classif.ksvm", "classif.knn", "classif.xgboost")
learners <- apply(learners,lrn)
learners <- c("classif.ksvm", "classif.knn", "classif.xgboost")
learners <- apply(learners,lrn)
learners <- lapply(learners,lrn)
learners <- c("classif.ksvm", "classif.kknn", "classif.xgboost")
learners <- lapply(learners,lrn)
id <- c("ksvm", "kknn", "xgboost")
?AutoTuner
auto_tuner <- AutoTuner$new(
learner = learner,
resampling = resampling,
measure = measure,
search_space = search_space,
terminator = terminator,
tuner = tuner
)
# define learners and ids
learners <- c("classif.ksvm", "classif.kknn", "classif.xgboost")
# obtain te selected learners
learners <- lapply(learners,lrn)
auto_tuner <- AutoTuner$new(
learner = learners,
resampling = resampling,
measure = measure,
search_space = search_space,
terminator = terminator,
tuner = tuner
)
auto_tuner <- lapply(learners, function(x) AutoTuner$new(
auto_tuner <- lapply(learners, function(x) AutoTuner$new(
learner = x,
resampling = resampling,
measure = measure,
search_space = search_space,
terminator = terminator,
tuner = tuner
))
knitr::opts_chunk$set(echo = TRUE)
```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(GGally)
library(reshape2)
library(mlr3verse)
library(mlr3extralearners)
library(mlr3pipelines)
# define the mutate pipe
mutate = po("mutate")
?mlr_pipeops_mutate
# define the mutate pipe
mutate = po("mutate")
