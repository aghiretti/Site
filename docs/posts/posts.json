[
  {
    "path": "posts/LMM/",
    "title": "An introduction to Linear Mixed Effect Models",
    "description": "",
    "author": [
      {
        "name": "Alessandro Ghiretti",
        "url": "https://example.com/"
      }
    ],
    "date": "2021-01-02",
    "categories": [
      "Statistics",
      "Mixed Effect Models"
    ],
    "contents": "\r\n\r\nContents\r\nLinear mixed effects modelsLinear mixed effects models theoryModel assumptions\r\n\r\nCorrelation structure for the random effects\r\nCovariance structure for the errors\r\nEstimationMaximum Likelihood estimation\r\n\r\nProfile Maximum Likelihood estimator\r\nRestricted Maximum Likelihood estimator (REML)\r\nTesting restrictions on parameters estimates\r\nLikelihood ratioTesting fixed effects linear restrictions\r\nTesting random effects restictions\r\n\r\nWald type statisticsTesting linear restriction on the fixed effects\r\n\r\nFitting linear models in R\r\n\r\n\r\nLinear mixed effects models\r\nLinear Mixed Effects models (LMM), are an extension of the standard linear regression model in which the residuals are normally distributed but may not be independent or have constant variance. As the name suggests LMM are models linear in the unknown parameters and are defined by two components the fixed and the random effects component. While it is customary to identify the fixed effects are represented by the unknown parameters associated with continuous covariates or with the level of a factor. On the other hand, random effects are used when the level of on or more factors are sampled randomly from an underlying population and inference needs to be extended to the whole population of levels. Fixed effects therefore describe the relationship between the depndent variable and a set of regressors for the entire population or for a relatively small number of subpopulations defined by the levels of a fixed factor for example the sex. On the contrary Random effects are random variables associated to the levels of a factor which represent random deviations from the population relationship described by the fixed effects.\r\nExamples of datataset that are analized with mixed effect models are\r\nClustered or hierachical data where units or observations are nested whitin clusters.\r\nLongitudinal or panel studies, where individuals or units are observed over time or under o situations in which data are collected through time under uncontrolled circumstances\r\nRepeated measurement studies, where individuals are monitored under controlled conditions through time or under different conditions\r\nClustered longitudinal or clustered repeated measurement studies, where individuals or units are clustered and observed at instants in time or under different conditions\r\nIn clustered data it is reasonable to assume that units belonging to the same cluster share some common characteristics or are commonly affected by some external factors and therefore exhibits correlation. In longitudinal and repeated measurement studies, the same subject or the same unit is observed repeatedly in time and therefore observations are the responses of the same unit are prone the exhibit a similar or correlated pattern that might be significantly different from that of the other individuals. Given their flexibility LMM become widely applied in many scientific fields, such as econometrics, social science, biology, psychologies.\r\nLinear mixed effects models theory\r\nFor the sake of simplicity we will consider a 2-level longitudinal data model where \\(y_{it}\\) represents the \\(t\\)th response for the \\(i\\)th subject.\r\n\\[y_{ti} = \\underbrace{x_{ti}' \\beta}_{fixed} + \\underbrace{z_{ti}' u_{ti}}_{random}  + \\epsilon_{ti} \\quad  \\quad i = 1, \\dots, m \\quad t = 1, \\dots, n_{i}.\r\n\\]\r\n\\(y_{ti}\\) is the response for subject \\(i\\) at instant\\(t\\), \\(x_{ti}\\) is a \\(k \\times 1\\) vector of regressors, \\(\\beta\\) a \\(k \\times 1\\) vector of fixed effect parameters, \\(z_{ti}\\) is the \\(q \\times 1\\) covariate vector of \\(j\\)th memeber of cluster \\(i\\)th for random effects and \\(u_{i}\\) the \\(q \\times 1\\) vector of random effect parameters and \\(\\epsilon\\) is the error term. By stacking the observations for the \\(i\\)th subject we obtain the subject-level representation as\r\n\\[\\begin{equation*}\r\n\\begin{pmatrix}\r\ny_{1i} \\\\\r\ny_{2i} \\\\\r\n\\vdots \\\\\r\ny_{n_{i} i}\r\n\\end{pmatrix}\r\n\r\n=\r\n\r\n\r\n\\begin{pmatrix}\r\nx_{1i}^{(1)} & x_{1i}^{(2)}& \\dots & x_{1n_{i}}^{(k)} \\\\\r\nx_{2i}^{(1)} & x_{2i}^{(2)} & \\dots & x_{2n_{i}}^{(k)} \\\\\r\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\r\nx_{n_{i}i}^{(1)} & x_{n_{i}i}^{(2)} & \\dots & x_{n_{i}i}^{(k)} \r\n\\end{pmatrix}\r\n\r\n\\begin{pmatrix}\r\n\\beta_{1} \\\\\r\n\\beta_{2}\\\\\r\n\\vdots\\\\\r\n\\beta_{k} \r\n\\end{pmatrix}\r\n\r\n+\r\n\r\n\\begin{pmatrix}\r\nz_{1i}^{(1)} & z_{1i}^{(2)} & \\dots & z_{1n_{i}}^{(q)} \\\\\r\nz_{2i}^{(1)} & z_{2i}^{(2)} & \\dots & z_{2n_{i}1}^{(q)} \\\\\r\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\r\nz_{n_{i}i}^{(1)} & z_{n_{i}i}^{(2)} & \\dots & z_{n_{i}i}^{(q)} \r\n\\end{pmatrix}\r\n\r\n\\begin{pmatrix}\r\nu_{1} \\\\\r\nu_{2}\\\\\r\n\\vdots\\\\\r\nu_{q} \r\n\\end{pmatrix}\r\n\\end{equation*}\\]\r\n\\[ Y_{i} = X_{i} \\beta +  Z_{i}U_{i}  + \\epsilon_{i} \\]\r\nThe \\(Z_{i}\\) matrix contains the same covariates of the \\(X_{i}\\) matrix with the difference that in general it has less elements. For random intercept model the \\(Z_{i}\\) matrix will be a vector of ones.\r\nFinally, stacking again the the observation level vectors and matrix we obtain the overall matrix formulation. That is,\r\n\\[ Y = X \\beta + ZU + \\epsilon \\]\r\nModel assumptions\r\nThorough the following we will assume that the following assumptions are satisfied\r\nAssumption A1: the error \\(\\epsilon\\) are normally distributed with mean \\(0\\) and covariance matrix \\(R_{i}\\)\r\n\\[ \\epsilon_{i} \\sim N(0,R_{i}) \\]\r\nAssumption A2; the random effects are normally distributed with mean \\(0\\) and covariance matrix \\(G\\)\r\n\\[ u_{i} \\sim N(0, G)\\]\r\nAssumption A3: the error terms associated to different subjects are indepent from eachother\r\n\\[ E(\\epsilon'_{i} \\epsilon_{i}) = 0 \\quad i,j = 1,2,\\dots,m \\]\r\nAssumption A4: the error terms \\(\\epsilon_{i}\\) and the fixed effects covariates \\(u_{i}\\) are uncorrelated\r\n\\[ E(u'_{j} \\epsilon_{i}) = 0 \\quad i,j = 1,\\dots,m \\] * Assumption A5: the error \\(\\epsilon_{i}\\) and the covariates are uncorrelated\r\n\\[ cov(u_{i},x_{j}) = 0 \\quad i,j = 1,2,\\dots,m\\]\r\nUnder assumption A1 and A2 the unconditional distribution of \\(Y_{i}\\) implied by the model is \\[ Y_{i}\\sim (X_{i}\\beta,\\: Z_{i}D Z_{i}' + R_{i})  \\]\r\nCorrelation structure for the random effects\r\nThe \\(D\\) matrix defines the correlation between random effects. When no constraints, apart the one of positive-definitneness and simmetry are imposed on the \\(D\\) matrix is said to be unstructured. In a 2-level LMM the unrestricted matrix is\r\n\\[\r\nD = var(u_{i}) = \r\n\\begin{pmatrix}\r\n\\sigma^{2}_{u1} & \\sigma_{u1,u2} \\\\\r\n\\sigma_{u1,u2} & \\sigma^{2}_{u2}\r\n\\end{pmatrix}\r\n\\] In this case the parameters to estimate are \\(\\theta_{D} = (\\sigma^{2}_{u1},\\sigma^{2}_{u2}, \\sigma_{u1,u2})\\) Other more parsimonious corvariance matrix can be obtained by imposing restrictions on \\(D\\) for example we might assume that the \\(D\\) matrix is diagonal,that is to say that random effects are not correlated.\r\n\\[\r\nD = var(u_{i}) = \r\n\\begin{pmatrix}\r\n\\sigma^{2}_{u1} & 0 \\\\\r\n0 & \\sigma^{2}_{u2}\r\n\\end{pmatrix}\r\n\\] Where \\(\\theta_{D} = (\\sigma^{2}_{u1},\\sigma^{2}_{u2})\\). Despite these are the two most common correlation forms in practice other specifications can be used. For example we can assume that the variances between two groups are different but the correlations within each group are the same\r\nCovariance structure for the errors\r\nThe simplest covariance matrix for matrix \\(R_{i}\\) is the diagonal structure where the errors of the same observations are assumed to be uncorrelated and to have equal varaince. For subject \\(i\\) the \\(R_{i}\\) has the following form\r\n\\[\r\nD = var(u_{i}) = \\sigma^{2}I_{n_{i}}\r\n\\begin{pmatrix}\r\n\\sigma^{2} & 0 & \\dots & 0 \\\\\r\n0 & \\sigma^{2} & \\dots & 0 \\\\\r\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\r\n0 & 0 & \\dots & \\sigma^{2}\r\n\\end{pmatrix}\r\n\\]\r\nThis is the most simple structure as it requires only one parameter to be estimated \\(\\theta_{R} = (\\sigma^{2})\\). When errors for the \\(i\\)th subject are assumed to be correlated a parsimlnious parametrization can be obtained with the compound symmetric matrix\r\n\\[\r\nD = var(u_{i}) = var(\\epsilon_{i})\r\n\\begin{pmatrix}\r\n\\sigma^{2} + \\sigma^{1} & \\sigma^{1} & \\dots & \\sigma^{1} \\\\\r\n0 & \\sigma^{2} + \\sigma^{1} & \\dots & \\sigma^{1} \\\\\r\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\r\n\\sigma^{1} & \\sigma^{1} & \\dots & \\sigma^{2} + \\sigma^{1}\r\n\\end{pmatrix}\r\n\\] in this case the errors associated to the \\(i\\)th unit are assumed have constant variance equal to ^{2} + ^{1} and equal covariance \\(\\sigma^{1}\\). In this case the parameters are \\(\\theta_{R} = (\\sigma^{2} + \\sigma^{1})\\). The compound symmetric structure is plausible when the correlation between errors of the same observation can be assumed constant. For example when repeated observations are performed under the same conditions. When the correlation structure is assumed to vary with time the first-order autoregressive structure, AR(1) is a preferable structure for \\(R_{i}\\). The AR(1) structure is \\[\r\nD = var(u_{i}) = var(\\epsilon_{i})\r\n\\begin{pmatrix}\r\n\\sigma^{2} & \\sigma^{2}\\rho & \\dots & \\sigma^{2}\\rho^{n_{i}-1} \\\\\r\n\\sigma^{2}\\rho & \\sigma^{2} & \\dots & \\sigma^{2}\\rho^{n_{i}-2} \\\\\r\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\r\n\\sigma^{2}\\rho^{n_{i}-1} & \\sigma^{2}\\rho^{n_{i}-2} & \\dots & \\sigma^{2}\r\n\\end{pmatrix}\r\n\\] with \\(\\theta_{R} = (\\sigma^{2}, \\rho)\\) and \\(\\rho \\in (-1,1)\\), The AR(1) structure assumes that the error variances for an observation ar all the same while the errors \\(w\\) times apart have covariance \\(\\sigma^{2}\\rho^{w}\\). Thsi structure implies that onbservations close in time are more correlated and that the correlation decay exponentially with the lag time.\r\nEstimation\r\nIn LMM the paramters to be estimated are: the fixed effect patameter \\(\\beta\\) and the covariance paramters \\(\\theta_{G}\\) and \\(\\theta_{R}\\). Therefore the parameters vector is $= (,{G}, {R}). We begin with maximum likelihood estimator and next we move to the other techniques\r\nMaximum Likelihood estimation\r\nHAving observed the data the likelihood contribution for the \\(i\\)th subject [referenza] is\r\n\\[ L_{i}(\\theta) = (2\\pi)^{\\frac{-n_{i}}{2}} det(V_{i})^{-\\frac{1}{2}} exp(-0.5 \\times (y_{i}-X_{i}\\beta)'V_{i}^{-1}(y_{i} -X_{i}\\beta))\\] where \\(V_{i}\\) was defined in [referenza]. The Likelihood is then defined of the product of the \\(m\\) marginal contributions\r\n\\[ L(\\theta) = \\prod_{i=1}^{m} L_{i}(\\theta) = \\prod_{i}^{m} (2\\pi)^{\\frac{-n_{i}}{2}} det(V_{i})^{-\\frac{1}{2}} exp(-0.5 \\times (y_{i}-X_{i}\\beta)'V_{i}^{-1}(y_{i} -X_{i}\\beta))\\]\r\nBy taking log of [Referenza] we obtain the corresponding log-likelihood\r\n\\[ \\ell(\\theta) = ln L(\\theta) = -0.5n \\times ln(2\\pi) - 0.5 \\times \\sum_{i=1}^{m} ln(det(V_{i})) -0.5 \\times \\sum_{i=1}^{m} ((y_{i}-X_{i}\\beta)'V_{i}^{-1}(y_{i} -X_{i}\\beta))\\]\r\nwhere \\(n = \\sum_{i=1}^{m} n_{i}\\). Despite it is possible to maximize [Referenza ] simultaneously for \\(\\beta\\) and \\(\\theta_{R}\\) and \\(\\theta_{D}\\) profile maximum likelihood is generally used especially when the number of parameters to be estimated is particularly large. Two cases can be considered for the maximization of[]. In maximizing [] two cases can be considered. The first assumes that the matrix \\(V_{i}\\) is known, but this hardly happens in practice. The second which is generally implemented in practice estimates both \\(\\beta\\) and \\(\\theta\\). When the vector \\(\\theta\\) and as a consequence the matrix \\(V_{i}\\) the log likelihood function becomes a function only of \\(\\beta\\) and the minimization of \\(- \\ell(\\theta)\\) reduces in the minimization of\r\n\\[ q(\\beta) = \\sum_{i=1}^{m} ((y_{i}-X_{i}\\beta)'V_{i}^{-1}(y_{i} -X_{i}\\beta)) \\]\r\nDifferentiating [] leads to\r\n\\[ \\hat{\\beta}_{GLS} = \\left( \\sum_{i=1}^{m} X_{i}' V_{i}^{-1} X_{i} \\right)^{-1} \\sum_{i=1}^{m} X_{i}'V_{i}^{-1} y_{i} \\] However the matrix \\(V_{i}\\) is not known in practice and alternative estimators must be used.\r\nProfile Maximum Likelihood estimator\r\nDespite maximizaion of the log-likelihood can be carried out contemporaneously for both \\(\\theta\\) and \\(\\beta\\) the profile maximum likelihood is a convenient tool that can be used especially when the number of parameters is large. The profile likelihood can be obtained by “profiling-out” the fixed effect parameters \\(\\beta\\) by substituting in [] the \\(GLS\\) expression. The resulting function is\r\n\\[ \\ell(\\theta)_{profile} =  -0.5n \\times ln(2\\pi) - 0.5 \\times \\sum_{i=1}^{m} ln(det(V_{i})) -0.5 \\times \\sum_{i=1}^{m} r_{i}' 'V_{i}^{-1} r_{i}\\] where \\(r_{i} = X_{i} \\left( \\left( \\sum_{i=1}^{m} X_{i}' V_{i}^{-1} X_{i} \\right)^{-1} \\sum_{i=1}^{m} X_{i}'V_{i}^{-1} y_{i} \\right)\\). the maximization of [] is a nonlinear optimization problem with positive-definiteness constraints imposed on the matrices \\(D\\) and \\(R_{i}\\). After obtaining the maximum likelihood estimators of \\(D\\) and \\(R_{i}\\), that is \\(\\hat{D}\\) and \\(\\hat{R_{i}}\\) we can use them to estimate the vector of fixed effects parameters \\(\\beta\\).\\ First we use \\(\\hat{D}\\) and \\(\\hat{R_{i}}\\) to estimate \\(V_{i}\\)\r\n\\[ \\hat{V_{i}} = Z_{i} \\hat{D}Z_{i} ' + \\hat{R}_{i} \\] nest, we can plug in \\(\\hat{V}_{i}\\) in [] to obtain an estimate for $. That is,\r\n\\[ \\hat{\\beta} = \\left( \\sum_{i=1}^{m} X_{i}' \\hat{V}_{i}^{-1} X_{i} \\right)^{-1} \\sum_{i=1}^{m} X_{i}'\\hat{V}_{i}^{-1} y_{i} \\] The resulting estimator is the best linear unbiased empirical estimator of \\(\\beta\\).\\ The main issue with this estimator [citazione ] is that the estimates of \\(\\theta\\) do not take into account the loss in the degrees of freedom resulting from estimating \\(\\beta\\), for this reason \\(REML\\) is commonly adopted in practice.\r\nRestricted Maximum Likelihood estimator (REML)\r\nREML estimator produces unbiased estimates of the covariance aprameters \\(\\theta\\) by taking into account the loss of degrees of freedom which results from estimating \\(\\beta\\).\\ The REML estimator is obtained by maximizing the restricted log-likelihood function\r\n\\[ \\ell(\\theta)_{restricted} =  -0.5(n-p) \\times ln(2\\pi) - 0.5 \\times \\sum_{i=1}^{m} ln(det(V_{i})) -0.5 \\times \\sum_{i=1}^{m} r_{i} 'V_{i}^{-1} r_{i} - 0.5 \\times \\sum_{i=1}^{m} ln(det(X'_{i}V_{i}^{-1}X_{i}))\\]\r\nOnce an estimate of \\(V_{i}\\) is obtained we can use formula [] to estimate \\(\\beta\\). Although the same formula is used to estimate the vector of fixed effects parameters the rulting anstiamtes and the associated variance will differe as the estimate of \\(b_{i}\\) obtained with the two technqiues is different.\\\r\nTesting restrictions on parameters estimates\r\nTesting parameters restriction in LMM is generally performed with likelihood ratio or Wald type statistics. When likelihood ratio is adopted the restrictions on the parameters are generally introduced with the concept of nested model, moreover the likelihood ratio requires the estimation of two different models the reference and the nested one. We will first introduce the two statistics and next expalin their use in testing restrictions on fixed effect and random effect parameters.\\\r\nLikelihood ratio\r\nThe likelihood ratio statistics is obtained by computing the lieklihood ratio of two models one nested in eachoter. The resulting test statistics is\r\n\\[ LR = -2 ln \\left(\\frac{L_{nested}}{L_{reference}} \\right) = -2 \\ell_{nested} -2 \\ell_{reference}. \\] Where \\(ell_{nested}\\) and \\(\\ell_{reference}\\) ar the likelihood evaluated at the parameters estimates of the two models. Under mild regularity conditions the \\(LR\\) statistics is asimptotically distributed as a chi-square distribution with degrees of freedom equals to the difference between the number of parameters in the reference and in the nested model.\r\nTesting fixed effects linear restrictions\r\nTo test restrictions on the fixed effects of the form\r\n$$ H_{0}: L = 0 \\ H_{1}: L 0\r\n$$ where \\(\\iota\\) is a \\(k \\times k\\) matrix containing ones depending on the restrictions to be tested. we can compute the LR ratio statistics and compare its result with a prescribed quantile of the \\(\\chi^{2}\\) distribution. If the \\(LR\\) statistics is larger than the Prescribed threshold value we reject \\(H_{0}\\).\r\nTesting random effects restictions\r\nWhen testing restriction of the covariance parameters of the random effects the \\(REML\\) should be preferred since, as stated previously they results in unbiased estimates for \\(\\theta\\). Depending on the fact that the covariance parameters under the null hypothesis lie or no on the boundary of the parameter space two cases must be discusses.\r\nCase 1: Under the null hypothesis the covarance parameters lie on the boundary of the patameter space. In this case we are testing if the random effect associate with unit \\(i\\) can be omitted from the model. Taht is,\r\n\\[ H_{0}: L D = 0 \\\\\r\nH_{1}: L D \\neq 0\r\n\\] where \\(\\iota\\) was defined previously. In this case the \\(LR\\) statistics is distributed a mixture of chi-squared distributions with \\(0\\) and \\(1\\); \\(LR \\sim 0.5 \\chi^{2}_{0} + 0.5 \\chi^{2}_{1}\\).\r\nCase 2. Under the null hypothesis the covariance parameters do not lie on the boundary of the parameter space In this case the likelihood ratio statistics follows a standard chi squared distribution with df equal to the difference between the number of parameters in the reference model and the number of parameters in the reduced model.\r\nWald type statistics\r\nThe main advantage of the likelihood ratio test is that only requires to know the likelihood function. However, two separate models needs to be fitted to test the linear restricitions. Converselly the Wald type statistic requires less computation as it only needs the reference model to be fitted For a single parameter \\(\\theta\\) a Wald type statistic is given by\r\n\\[\r\nW_{stat} = \\frac{(\\hat{\\theta} - \\theta_{0})^{2}}{var(\\hat{\\theta})}\r\n\\] where \\(var(\\hat{\\theta})\\) is the variance associated to \\(\\hat{\\theta}\\) and \\(\\theta_{0}\\) is the parameter value under the null hypothesis \\(H_{0}\\). When \\(\\theta_{0} = 0\\) the statistic reduces to\r\n$$ W_{stat} = \r\n$$\r\nA Wald type statistic is asimptotically equivalent to the \\(LR\\) statistic and has a chi-squared distribution with one degree of freedom. That is \\(W_{stat} \\sim \\chi^{2}_{1}\\)\r\nTesting linear restriction on the fixed effects\r\nFitting linear models in R\r\nMarketing mix modelling aims to understand the effect that different marketing variables or campaigns have on the sales or on other quantities of interest. Depending on the aggregation level of a marketing mix model can be develop to study the marginal effect of the marketing variables on the product, the category, the shop, the area etc. Given the hierarchy of data mixed effect models can therefore applied to marketing mix modelling when a sample of categories, products or shop is considered. In this example we will consider the sales of 6 products coming from 3 different In marketing or retailing it is of primary importance to evaluate the marginal effect that promotions or other activities have on sales or other targeted variables. For example, a retailer might be interested in estimating the effects that a series of activites such as price promotions and advertising had on the average sales of the categories over a certain period of time. Since retailers data clustere at different levels: regions, cities, shops, categories, products and time it is reasonable to assume that shops in the same city as well as products in the same category are correlated. LMM can be used to account for this correlation.\r\nWe begin the analysis by loading the lmer and the other packages needed for the analysis. Desptie lme4 is generally used lmer allows for the specification of more detailed correlation structures.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-01-02T14:55:09+01:00",
    "input_file": {}
  },
  {
    "path": "posts/DynamicMMM/",
    "title": "Applying Dynamic Linear Regression to Marketing Mix Modelling",
    "description": "",
    "author": [
      {
        "name": "Alessandro",
        "url": {}
      },
      {
        "name": "",
        "url": {}
      }
    ],
    "date": "2021-01-02",
    "categories": [
      "Econometrics",
      "State Space Models",
      "Time Series"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nState Space ModelsRegression with time varying parameters\r\n\r\nDynamic Marketing Mix ModelData description\r\nPreliminary Analysis\r\nMedia mix model specification\r\nFitting the model with dlm\r\nElasticities\r\nConclusions\r\n\r\n\r\nIntroduction\r\nMarketing mix modeling is a statistical analysis that links multiple variables, including marketing, sales activities, operations and external factors, to changes in consumer behavior, such as acquisition, sales, revenue, and retention. It can then support the development of forwardlooking business simulations and optimization exercises. When only advertising investement is considered marketing mix modeling is generally called media mix modeling.\r\nOne of the models more used in practice for makreting and media mix modeling is the linear regression model. Once the regression is fitted and estimates are obtained for the unknown parameter, it is possible to infer the effect that the regressors have on the response variable and obtain key informations such as marginal estimated contributions and elasticities.\r\nDespite linear regression represent a model commonly adopted in practice because of its flexibility and simplicity, it is important to remark that when dealing with time series data several details must to be kept in mind when constructing the model. For example spurious correlations between predictors and the response, correlation structure in the errors terms, structural breaks, seasonality and so on.\r\nOne of the key assumptions about the plain linear regression model is that the estimates of the parameters remain constant over the period considered. This assumption appears to be limited in many context, for example in marketing it might be assumed that the marginal effect on budget allocated to different media changes, as the customer behavior or the penetration of one media evolves over time.\r\nIn order to overcome this last limitation the Dynamic Linear Regression Model, which allows for time varying parameters can be adopted. Once the DLM is casted in State Space form it is possible to estimate the parametersm in a dynamic way.\r\nWe will first give an introduction to the State Space models, and then we will show, using the R package dlm, how to fit a DLM to a Media Mix Modeling problem.\r\nState Space Models\r\nState space models provide a unified methodology for treating a wide range of problems in time series analysis. In this approach it is assumed that the development of the time series under study is determined by an unobserved component \\(\\alpha_{1},\\dots,\\alpha_{n}\\), with which are associated a series of observations \\(y_{1},\\dots,y_{n}\\); the relation between the \\(\\alpha_{t}\\)’s and the \\(y_{t}\\)’s is specified by the state space model. The main purpose of state space analysis is to infer the relevant properties of the \\(\\alpha_{t}\\)’s from a knowledge of the observations \\(y_{1},\\dots,y_{n}\\). The simple linear gassian state space model is givne by\r\n\\[\r\n\\begin{aligned}\r\ny_{t} &=Z_{t} \\alpha_{t}+\\varepsilon_{t}, & & \\varepsilon_{t} \\sim \\mathrm{N}\\left(0, H_{t}\\right) \\\\\r\n\\alpha_{t+1} &=T_{t} \\alpha_{t}+R_{t} \\eta_{t}, & & \\eta_{t} \\sim \\mathrm{N}\\left(0, Q_{t}\\right), & t=1, \\ldots, n,\r\n\\end{aligned}\r\n\\] where \\(y_{t}\\) is a \\(p \\times 1\\) vector of observations called the observation vector and \\(\\alpha_{t}\\) is an unobserved \\(m \\times 1\\) vector called the state vector. The idea underlying the model is that the development of the system over time is determined by \\(\\alpha_{t}\\) according to the second equation of, but because \\(\\alpha_{t}\\) cannot be observed directly we must base the analysis on observations \\(y_{t}\\). The first equation of is called the observation equation and the second is called the state equation. The matrices \\(Z_{t}, T_{t}, R_{t}, H_{t} and Q_{t}\\) are initially assumed to be known and the error erms \\(\\epsilon_{t}\\) and \\(\\eta_{t}\\)t are assumed to be serially independent and independent of each other at all time points. In practice, some or all of the matrices \\(Z_{t}, T_{t}, R_{t}, H_{t} and Q_{t}\\) will depend on elements of an unknown parameter vector \\(psi\\) that needs to be estimated. Estimation of \\(\\psi\\) is generally performed by maximising the prediction error decomposition form of the likelihood\r\n\\[\r\n\\log L\\left(Y_{n}\\right)=-\\frac{n p}{2} \\log 2 \\pi-\\frac{1}{2} \\sum_{t=1}^{n}\\left(\\log \\left|F_{t}\\right|+v_{t}^{\\prime} F_{t}^{-1} v_{t}\\right)\r\n\\]\r\nIn order to obtain the prediction error decomposition of the likelihood the Kalman Filter is used. The Kalman Filter consists of the following set of recursive equations.\r\n\\[\r\n\\begin{aligned}\r\nv_{t} &=y_{t}-Z_{t} a_{t}, & F_{t} &=Z_{t} P_{t} Z_{t}^{\\prime}+H_{t} \\\\\r\na_{t \\mid t} &=a_{t}+P_{t} Z_{t}^{\\prime} F_{t}^{-1} v_{t}, & P_{t \\mid t} &=P_{t}-P_{t} Z_{t}^{\\prime} F_{t}^{-1} Z_{t} P_{t} \\\\\r\na_{t+1} &=T_{t} a_{t}+K_{t} v_{t}, & P_{t+1} &=T_{t} P_{t}\\left(T_{t}-K_{t} Z_{t}\\right)^{\\prime}+R_{t} Q_{t} R_{t}^{\\prime}\r\n\\end{aligned}\r\n\\]\r\nThe application of the Kalman filter requires that the inizial condition of the systems \\(\\alpha_{1} \\sim N(a_{1},P_{1})\\) are known. This rarely happens in practice and when the initial conditions of the system are not known the diffuse intiialization is adopted. In practice the diffuse inizialization consists in initializig the distribution of \\(\\alpha_{1}\\) with a high variance. When the diffuse initialization is adopted the resulting likelihood is\r\n\\[\r\n\\log L_{d}\\left(Y_{n}\\right)=-\\frac{n p}{2} \\log 2 \\pi-\\frac{1}{2} \\sum_{t=1}^{d} w_{t}-\\frac{1}{2} \\sum_{t=d+1}^{n}\\left(\\log \\left|F_{t}\\right|+v_{t}^{\\prime} F_{t}^{-1} v_{t}\\right)\r\n\\]\r\nwhere \\(w_{t}\\) is a variable related to the.\r\nRegression with time varying parameters\r\nState Space form represent a natural way to represent a linear regression with time varying parameters . The observation equation is formulated as\r\n\\[ y_{t} = x'_{t}\\beta_{t} + \\epsilon_{t} \\quad \\epsilon_{t} \\sim N(0,\\sigma^{2})\\]\r\nwhere \\(y_{t}\\) is the response variable observed at time \\(t\\), \\(x_{t}\\) is a \\(k \\times 1\\) vector of regressors, \\(\\beta_{t}'\\) is a \\(k \\times 1\\) vector of time varying parameters and \\(\\epsilon_{t}\\) is the error term.\r\nIn order to describe the behavior of the parameters the a state equation must be specified. If the parameters \\(\\beta_{t}\\) are assumed follow a random walk model the full specification of the DLM becomes\r\n\\[\r\n\\begin{aligned}\r\ny_{t} &= x'_{t} \\beta_{t}+\\varepsilon_{t} & & \\varepsilon_{t} \\sim \\mathrm{N}\\left(0, \\sigma^{2}_{t}\\right) \\\\\r\n\\beta_{t+1} &= \\beta_{t}+\\eta_{t} & & \\eta_{t} \\sim \\mathrm{N}\\left(0, Q_{t} \\right) & t=1, \\ldots, n,\r\n\\end{aligned}\r\n\\] where \\(T_{t} = I\\) and \\(Z_{t} = x'_{t}\\). The behavior of the parameters is guided by the matrix \\(Q\\), when \\(Q = 0\\) the model reduces the standard static linear regression model. Depending on the underlying assumptions or evidence other stochastic processes can be used to model the bahvior of the parameters \\(\\beta_{t}\\). For example when cyclical behavior is observed an autoregressive process of order 2, can be used to model the parameters.That is,\r\n\\[ \r\n\\begin{aligned}\r\ny_{t} &= x'_{t} \\beta_{t}+\\varepsilon_{t}  \\quad &\\varepsilon_{t} \\sim \\mathrm{N}\\left(0, \\sigma^{2}_{t}\\right) \\\\\r\n\\beta_{t+1} &= \\phi_{0} + \\phi_{1}\\beta_{t-1} + \\phi_{2} \\beta_{t-2} + \\eta_{t} \\quad &\\eta_{t} \\sim N(0,Q). \r\n\\end{aligned}\r\n\\]\r\nThe specification of the dynamic behavior of the parameters is generally guided by underlying assumptions or empirical evidence of the phenomena under study.\r\nDynamic Marketing Mix Model\r\nData description\r\nThe data contains sales (expressed in thousand of units) and the corresponding advertising budget (expressed in thousand of dollars) for TV, Radio and Newspaper. The data ranges from 2000/01/01 to 2016/08/01 for a total of 200 observations.\r\nPreliminary Analysis\r\n\r\n\r\n\r\n\r\n\r\n\r\nA time series plot reveals that there is a great difference between the budget allocated to the different channels. TV represents the media to which more money are allocated, followed by Newspaper and Radio. With scatterplot matrix we inspect the relationship between the budget allocated to the different channels and the volume of sales.\r\n\r\n\r\n\r\nIt appears that there is an high correlation between sales and the investment in TV advertising, with a correlation coefficient of 0.782. The correlation with the other two media , Radio and Newspaper is lower, with correlation coefficients of 0.576, 0.228. Furthermore, the scatterplot of the sales with the TV and Radio budget shows heteroskedasticity in the data. Despite the lack of a strong linear relationship both with Radio and Newspaper and the presence of non constant variance we proceed with our model specification.\r\nMedia mix model specification\r\nA general model specification for media mix modeling is the standard linear regression model,\r\n\\[ y_{t} = \\mu_{t} + \\gamma_{t} + x_{t}'\\beta + \\epsilon_{t} \\quad \\epsilon_{t} \\sim N(0,\\sigma^{2}) \\]\r\nwhere \\(\\mu_{t}\\) is the trend component representing the long run beahvior of the series, \\(\\gamma_{t}\\) is the seasonal component, \\(x'_{t}\\) is the vector of covariates, \\(\\beta\\) is the vector of unknown parameter and \\(\\epsilon_{t}\\) is the error term. When no seasonal behavior or trend are detected or assumed in the series the model reduces to\r\n\\[ y_{t}  = x'_{t}\\beta + \\epsilon_{t} \\quad \\epsilon_{t} \\sim N(0,\\sigma^{2}).\\]\r\nAs stated in the introduction one limitation of the plain linear regression model is that the marginal effects of the explanatory variables, represented by \\(\\beta\\), remains constant over time. Nevertheless, in some context, it might be more natural to allow the vector \\(\\beta\\) to evolve with time. In this example we will consider the following model\r\n\\[\r\n\\begin{aligned}\r\ny_{t} &= x_{t}'\\beta_{t} + \\epsilon_{t} \\quad &\\epsilon_{t} \\sim N(0,\\sigma^{2}) \\\\\r\n\\beta_{t+1} &= \\phi_{0} + \\phi_{1}\\beta_{t-1} + \\eta_{t}  \\quad &\\eta_{t} \\sim N(0, Q)\r\n\\end{aligned}\r\n\\]\r\nwhere the behavior of the parameters is modeled as a random walk process.\r\nFitting the model with dlm\r\nThe package that we will use to fit the dynamic linear model is dlm, developed by Giovanni Petris. The package provides a flexible environment for fitting Dynamic Linear Gaussian models. Before to be fitted the data needs to be transformed into a ts object. Since the data are monthly we set frequency equal to 12.\r\n\r\n\r\n# convert the data to a ts object\r\nadv.ts <- ts(advertising, start = advertising$t[1], frequency = 12)\r\n\r\n\r\n\r\nNext, we extract the response variable (sales) and the explanatory variables, and proceed to fit the model and obtain the filtered and the smoothed states.\r\n\r\n\r\nx <-  adv.ts[,1:3]\r\ny <-  adv.ts[,4] \r\n\r\n# Define the linear regression model\r\n# Logaritmic transofrmation is applied to variance components to increase\r\n# the stability of the Kalman Filter\r\nbuildTVREG <-  function(parm, x.mat){\r\n  # parametrize the model as log(variance)\r\n  parm <- exp(parm)\r\n  # return function output\r\n  return(dlmModReg(X = x.mat, dV = parm[1], dW = parm[2:5]))\r\n}\r\n\r\n# Set initial values for optimizer\r\nstart.values = rep(0,5)\r\nnames(start.values) = c(\"lns2V\",\"lns2a\", \"lns2TV\", \"lns2Radio\", \"lns2Newspaper\")\r\n\r\n# Obtaiun maximum likelihood estimates\r\nTVREG.mle = dlmMLE(y = y, parm = start.values, x.mat = x, buildTVREG)\r\n\r\n\r\n\r\nAfter having obtained the estimates of the parameters we check that the convergence criterion was met\r\n\r\n\r\nTVREG.mle$convergence\r\n\r\n\r\n[1] 0\r\n\r\nA zero values represent that the optimizer converged. The resulting estimated variances are\r\n\r\n        lns2V         lns2a        lns2TV     lns2Radio lns2Newspaper \r\n 2.736214e+00  1.870344e-06  1.038898e-11  9.426244e-06  5.724634e-09 \r\n\r\nThe estimated variances can be built into the model to calculate the filtered and the smoothed states, which in our SS formulation correspond to the unknown parameters.\r\n\r\n\r\n# Assign parameters to the estimated model\r\nTVREG.dlm <- buildTVREG(TVREG.mle$par,x)\r\n\r\n# Obtain fitlered states\r\nTVREG.f <- dlmFilter(y,TVREG.dlm)\r\n\r\n# Obtain smoothed states\r\nTVREG.s <-  dlmSmooth(TVREG.f)\r\n\r\n\r\n\r\nOnce the smoothed states are calculated we can plot them over the period considered.\r\n\r\n\r\n# Etract smoothed states\r\ns = TVREG.s$s\r\n\r\n# Assign column names to the smoothed states\r\ncolnames(s) <- c(\"State_Intercept\",\"State_TV\",\"State_Radio\",\"State_Newspaper\")\r\n\r\n# Plot the smoothed states\r\ns  %>% as_tibble() %>% \r\n  filter(between(row_number(),1,n()-1)) %>% \r\n  mutate(t = t) %>% \r\n  pivot_longer(cols = -c(\"t\"), names_to = \"pars\") %>% \r\n  ggplot(aes (x = t, y = value, color = pars)) +\r\n  geom_line() +\r\n  ylab(\"Smoothed State\") +\r\n  facet_wrap(.~pars, scales = \"free\")\r\n\r\n\r\n\r\n\r\nThe smoothed intercept represent the expected volume of sales when no budget is allocated in media investment while the other states represent the unitary increase in volume of sales when another dollar is allocated to the corresponding media channel. Plot ?? shows all the smoothed states over the period considered. From 2001 to 2005 the intercept shows an light upward trend followed by a marked negative trend and a slight recovery during the last months. The parameter of the newspaper shows a marked positive trend. Importantly it switches from a negative to a positive sign. Meaning that while at the beginning of the period considered investing in newspaper had a negative effect on sales now it has a positive effect. The marginal effect of Radio shows an unstable behavior with a marked drop after 2010 followed by a rapid increase. Finally, the marginal effect of the TV budget shows a steady negative trend meaning that the effect of TV advertising is decreasing.\r\nElasticities\r\nThe estimated model is given by\r\n\\[ \r\n\\widehat{sales}_{t}  = \\hat{\\beta}_{0t} +  TV_{t} \\times \\hat{\\beta}_{1t} + Radio_{t} \\times  \\hat{\\beta}_{2t} + Newspaper_{t} \\times \\hat{\\beta}_{3t}  \r\n\\]\r\nwhere each \\(\\beta_{t}\\) is modeled as a random walk process. The elasticity can next be obtained as\r\n\\[\r\nE_{jt} = \\frac{\\Delta y}{\\Delta x} \\cdot \\frac{x}{y} = \\beta_{jt} \\frac{x}{y}.\r\n\\]\r\nTo calculate the elasticities we substitute for \\(x\\) and \\(y\\) their means. Another common approach to estimate elasticities with regression models is to adopt a double log transformation, which result in a model of the form\r\n\\[\r\nlog(y_{t}) = log(x'_{t})\\beta + \\epsilon_{t}\r\n\\] Under this transformation the estimated parameter \\(\\hat{\\beta}\\) correspond to the value of the elasticity.\r\nThe resulting elasticties obtained from the smoothed states are shown in figure ()\r\n\r\n\r\n# Obtain the elasticities from the smoothed states\r\nelasticities <- s %>% as_tibble() %>%\r\n  filter(between(row_number(),1,n()-1)) %>% \r\n  cbind(advertising) %>% \r\n  mutate(TV_el = State_TV*(mean(TV)/mean(Sales)),\r\n         Radio_el = State_Radio*(mean(Radio)/mean(Sales)),\r\n         Newspaper_el = State_Newspaper*(mean(Newspaper)/mean(Sales))) %>%\r\n  select(t,contains(\"el\")) %>% \r\n  rename(\"TV\" = \"TV_el\", \"Radio\" = \"Radio_el\", \"Newspaper\" = \"Newspaper_el\")\r\n           \r\n# Plot the elasticities\r\nelasticities %>% \r\n  pivot_longer(cols = -c(\"t\"), names_to = \"Elasticities\") %>% \r\n  ggplot(aes (x = t, y = abs(value), color = Elasticities)) +\r\n  geom_line() +\r\n  ylab(\"Elasticities\") +\r\n  facet_grid(rows = vars(Elasticities), scales = \"free\")\r\n\r\n\r\n\r\n\r\nFrom the plot it appears that the volume of sales is unelastic to all the medias considered in the model. TV represent the channel with the higher elasticity, however as it show in the plot the elasticity exhibits a decreasing behavior. The elasticities associated to Radio and Newspaper despite being lower compared to the TV have show a marked upward trend, meaning that in the future these media might represent the best channels to which allocate budget to increase sales.\r\nFinally we can forecast the future states for the next twelve months and calculate the corresponding elasticities.\r\n\r\n          [,1]       [,2]         [,3]        [,4]\r\n [1,] 6.782885 0.04809596 -0.001938591 0.004793487\r\n [2,] 6.782885 0.04809596 -0.001938591 0.004793487\r\n [3,] 6.782885 0.04809596 -0.001938591 0.004793487\r\n [4,] 6.782885 0.04809596 -0.001938591 0.004793487\r\n [5,] 6.782885 0.04809596 -0.001938591 0.004793487\r\n [6,] 6.782885 0.04809596 -0.001938591 0.004793487\r\n [7,] 6.782885 0.04809596 -0.001938591 0.004793487\r\n [8,] 6.782885 0.04809596 -0.001938591 0.004793487\r\n [9,] 6.782885 0.04809596 -0.001938591 0.004793487\r\n[10,] 6.782885 0.04809596 -0.001938591 0.004793487\r\n[11,] 6.782885 0.04809596 -0.001938591 0.004793487\r\n[12,] 6.782885 0.04809596 -0.001938591 0.004793487\r\n\r\nHaving modeled the states as random walk does not provide much predictive power. With more structure imposed on the states such as a random walk with drift or an autoregressive process better informative forecasts can in principle be obtained.\r\nConclusions\r\nIn this example we showed how state space models can be used to augment the information provided by static time series models. The state space formulation is general and can be adopted for many different models. With it and the Kalman Filter smoothing, filtering and predicting can be performed. In this particular example the analysis of the smoothed states in the Dynamic Linear Regression showed how the marginal effect of the different media changed over the period considered. It is important to note that the behavior of the estimated parameters might also be a symptom of mispecification. In fact, as more variable are added to the model or a different specification is suggested the smoothed states might result in constant values. Moreover, in this particular case the estimated variances resulted particularly small and the variation observed in the smoothed state could be negligible for practical purposes.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-01-02T23:35:14+01:00",
    "input_file": "State_Space_MMM.utf8.md"
  },
  {
    "path": "posts/SURE/",
    "title": "Seemengly Unrelated Regression Equations (SURE)",
    "description": "In this post we give an introduction to the theory behind SURE models.",
    "author": [
      {
        "name": "Alessandro Ghiretti",
        "url": "https://example.com/"
      }
    ],
    "date": "2021-01-02",
    "categories": [
      "Econometrics",
      "Regression"
    ],
    "contents": "\r\n\r\nContents\r\nThe SURE modelModel assumptions\r\n\r\nEstimationDisturbances are uncorrelated and the regressors are different across all the equations\r\nContemporaneously uncorrelated disturbances\r\nIdentical regressors\r\nMaximum Likelihood estimation\r\n\r\nTesting linear and nonlinear restrictions\r\nTesting wether \\(\\Omega\\) is diagonal\r\nCategory sales sensitivity to price\r\n\r\nThe SURE model\r\nSuppose that \\(y_{it}\\) is a dependent variable, \\(x_{it} = (1, x_{it1}, x_{it2}, ..., x_{itK−1})\\) is a \\(K_{i}\\)-vector of explanatory variables for observational unit \\(i\\), and \\(u_{it}\\) is an unobservable error term, where the double index \\(it\\) denotes the \\(t\\)th observation of the \\(i\\)th equation in the system. A classical SURE model is given by the system of linear regression equations \\[y_{i} = X_{i}\\boldsymbol{\\beta}_{i} + u_{i}, \\quad i = 1,2,\\dots,m \\] where \\(y_{i} = (y_{i1}, y_{i2},\\dots,y_{iT})'\\) is a \\(T \\times 1\\) vector on the dependetn variable \\(y_{it}\\) and \\(X_{it}\\) is a \\(T \\times k_{i}\\) matrix of observations on the \\(k_{i}\\) vector of regressors explaining \\(y_{it}\\), \\(\\beta_{i}\\) is a \\(k \\times 1\\) vector of unknonw coefficients and \\(u_{i} = (u_{i1}, u_{i2}, \\dots, u_{iT})\\) is a \\(T \\times 1\\) vecotr of disturbances. By stacking the the different equations in the system as\r\n\\[\\begin{equation*}\r\n\\begin{pmatrix}\r\ny_{1}\\\\\r\ny_{2} \\\\\r\n\\vdots\\\\\r\ny_{m}\\\\\r\n\\end{pmatrix}\r\n=\r\n\\begin{pmatrix}\r\nX_{1} & 0 & \\cdots & 0\\\\\r\n0 & X_{2} & \\cdots & 0 \\\\\r\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\r\n0 & 0 & \\cdots & X_{m} \\\\\r\n\\end{pmatrix}\r\n\r\n\\begin{pmatrix}\r\n\\beta_{1}\\\\\r\n\\beta_{2} \\\\\r\n\\vdots\\\\\r\n\\beta_{m}\r\n\\end{pmatrix}\r\n+\r\n\\begin{pmatrix}\r\nu_{1}\\\\\r\nu_{2} \\\\\r\n\\vdots\\\\\r\nu_{3}\r\n\\end{pmatrix}\r\n\\end{equation*}\\]\r\nwe can obtain the model written in compact notation \\[ y = G \\beta + u\\]\r\nModel assumptions\r\nIn order to be able to estimate the model parameters and obtain consistent and unbiased estimates some assumptions must be introduced.\r\nAssumption A1:The vector of disturbances \\(u\\) has zero conditional mean\r\n\\[ E(u|X_{1},X_{2},\\dots,X_{m}) = 0\\]\r\nAssumption A2:The disturbances are uncorrelated across observations\r\n\\[ E(u_{i}u'_{i}|X_{1},X_{2},\\dots,X_{m}) = \\sigma^{2}_{ii} I_{T}\\]\r\nIn particular three cases can be considered 1. The disturbances are contemporaneously uncorrelated: \\(E(u_{i}u'_{j}) = 0\\) for \\(i \\neq j\\)\r\nThe disturbances are uncorrelated and the regressors are identical across all the equations\r\n\\(E(u_{i}u'_{j}) = \\sigma_{ij} I_{T}\\) and \\(X_{i} = X_{j}\\) for all \\(i,j\\)\r\nThe disturbances are uncorrelated and the regressors are different across all the equations\r\n\\(E(u_{i}u'_{j}) = \\sigma_{ij} I_{T}\\) and \\(X_{i} \\neq X_{j}\\) for some \\(i,j\\)\r\nEstimation\r\nEstimates for the unknonwn parameters \\(\\beta\\) can be obtained with different estimation procedures. The most common one are the Maximum Likelihood (\\(ML\\)) and the Generalized Least Squares (\\(GLS\\)) estimator. Here we will consider the \\(GLS\\) estimator. Good reference for \\(ML\\) estimation is given in [referenze]. The three different cases discussed above will be discussed separately. Following [PESARAN] we start from the more general case 3, that is the disturbances are contemporaneously uncorrelated and the regressors are different across all the equations\r\nDisturbances are uncorrelated and the regressors are different across all the equations\r\nUnder the assumption that \\(E(u_{i}u_{j}'|X_{1}, X_{2},\\dots, X_{m}) = \\sigma_{ij} I_{T}\\) we have that \\[ E(uu'|X_{1}, X_{2},\\dots,X_{m}) = \\Omega = \\Sigma \\otimes I_{T} \\] where \\(\\Sigma = (\\sigma_{ij})\\) is an \\(m \\times m\\) symmetric posivie definite matrix. When \\(\\Sigma\\) is known an efficient estimator of \\(\\beta\\) is the \\(GLS\\) estimator, that is\r\n\\[ \\hat{\\beta}_{GLS} = [G'(\\Sigma^{-1} \\otimes I_{T})G]^{-1}G'(\\Sigma^{-1} \\otimes I_{T}) y \\]\r\nwith asymptotic covariance matrix\r\n\\[ var(\\hat{\\beta}_{GLS}) = [G'(\\Sigma^{-1} \\otimes I_{T}) G]^{-1}. \\]\r\nIn practice \\(\\Sigma\\) is not known and a feasible generalzied least squares \\(FGLS\\) estimator is obtained by replacing \\(\\Sigma\\) with a suitable estimate. When \\(m\\) is small with respect to \\(T\\) two stage least squares is generally adopted. \\(2SLS\\) proceeds as follows:\r\nCompute \\(\\hat{\\beta}_{OLS} = (G'G)^{-1}G'y\\)\r\nOtain the residuals \\(\\hat{u} = y - G\\hat{\\beta}_{OLS}\\)\r\nEstimate the elements of \\(\\Sigma\\) as \\(\\hat{\\sigma}_{ij} = \\frac{\\hat{u}'_{i}\\hat{u}_{j}}{T}\\) for \\(i,j = 1,2,\\dots,m\\)\r\nThe final \\(FGLS\\) is then obtained substituting \\(\\hat{\\Sigma}\\) in the \\(GLS\\) equation, leading to:\r\n\\[ \\hat{\\beta}_{FGLS} = [G'(\\hat{\\Sigma}^{-1} \\otimes I_{T})G]^{-1}G'(\\hat{\\Sigma}^{-1} \\otimes I_{T}) y \\] however when \\(m\\) is large other estimators for \\(\\Sigma\\) are preferred [REFERENZA CAP 29 PESARAN].\r\nContemporaneously uncorrelated disturbances\r\nWhen the disturbances are contemporaneously uncorrelated, that is \\(E(u_{i}u'_{j}) = 0\\) for \\(i \\neq j\\), there is no gain by considering the equations in (REFERENZA) as a system and the application of single equation estimators will yield efficient estimators\r\n\\[\\begin{equation*}\r\n\\hat{\\beta}_{GLS}=\r\n\r\n\\begin{pmatrix}\r\n\\left(X_{1}' X_{1}\\right)^{-1} & 0 & \\ldots & 0 \\\\\r\n0 & \\left(X_{2}^{\\prime} X_{2}\\right)^{-1} & \\ldots & 0 \\\\\r\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\r\n0 & 0 & \\ldots & \\left(X_{m}' X_{m}\\right)^{-1}\r\n\\end{pmatrix}\r\n\r\n\r\n\\begin{pmatrix}\r\nX_{1}' y_{1} \\\\\r\nX_{2}' y_{2} \\\\\r\n\\vdots \\\\\r\nX_{m}' y_{m}\r\n\\end{pmatrix}\r\n\r\n\r\n=\r\n\r\n\\begin{pmatrix}\r\n\\hat{\\beta}_{1, OLS} \\\\\r\n\\hat{\\beta}_{2, OLS} \\\\\r\n\\vdots \\\\\r\n\\hat{\\beta}_{m, OLS}\r\n\\end{pmatrix}\r\n\\end{equation*}\\]\r\nIdentical regressors\r\nIn the case of identical regressors the matrix \\(G\\) can be written as \\(G = I_{m} \\otimes X\\). With some algebra in can be shown that the \\(GLS\\) estimator can be written as\r\n\\[\\begin{equation*}\r\n\\begin{split}\r\n\\hat{\\beta}_{GLS} & = \\left[ \\left( I_{m} \\otimes X' \\right) \\left( \\Sigma^{-1} \\otimes I_{T} \\right) \\left(I_{m} \\otimes X \\right)\\right]' \\left(I_{m} \\otimes {X}' \\right) \\left(\\Sigma^{-1} \\otimes I_{T} \\right) y \\\\\r\n\r\n& =\\left(\\Sigma^{-1} \\otimes X' X \\right)^{-1} \\left(\\Sigma^{-1} \\otimes X' \\right) y = \\left[ \\Sigma \\otimes\\left(X' X \\right)^{-1} \\right] \\left(\\Sigma^{-1} \\otimes X' \\right) y \\\\\r\n\r\n& =\\left[ I_{m} \\otimes\\left(X' X \\right)^{-1} X' \\right] y \\\\\r\n\r\n& =\\left( \\hat{\\beta}_{1, OLS}', \\hat{\\beta}_{2, OLS}', \\ldots, \\hat{\\beta}_{m, OLS} \\right)'\r\n\r\n\\end{split}\r\n\\end{equation*}\\]\r\nTherefore, when all the equations have the same regressors in common the \\(GLS\\) reduces to the leas square procedure applied to one equation at a time.\r\nMaximum Likelihood estimation\r\nUnder the assumption that \\(u_{t} \\sim \\operatorname{IID} N(0, \\Sigma), t=1,2, \\ldots, T\\) the log-likelihood function of the stacked system [Riferimento] can be written as\r\n\\[\\begin{equation} \\label{loglike}\r\n\\ell(\\theta)= -\\frac{T m}{2} \\log(2 \\pi)-\\frac{T}{2} \\log |\\Sigma|-\\frac{1}{2}(y-G \\beta)' \\left(\\Sigma^{-1} \\otimes I_{T} \\right)(y-G \\beta)\r\n\\end{equation}\\]\r\nwhere we used the fact that \\(\\Omega = \\Sigma \\otimes I_{T}\\).\r\nDifferentiating \\(\\ref{loglik}\\) with resepct to the unknown parameters lead to the \\(ML\\) estimators\r\n\\[\\begin{equation} \\label{ml1}\r\n\\tilde{\\sigma}_{i j}=\\frac{\\left(y_{i} - X_{i} \\tilde{\\beta}_{i}\\right)' \\left(y_{j}-X_{j} \\tilde{\\beta}_{j}\\right)}{T}\r\n\\end{equation}\\]\r\n\\[\\begin{equation} \\label{ml2}\r\n\\tilde{\\beta}=\\left[G' \\left(\\tilde{\\Sigma}^{-1} \\otimes I_{T} \\right) G \\right]^{-1} G' \\left(\\tilde{\\Sigma}^{-1} \\otimes I_{T} \\right) y\r\n\\end{equation}\\]\r\nIn order to obtain the maximum likelihood estimates several approaches can be adopted such as the profile likelihood which consists in first maximizing the log-likelihood with respect to \\(\\Sigma\\) and then substituting \\(\\hat{\\Sigma}\\) into  and maximize it with respect to \\(\\beta\\) or the iterative least squares \\((IRLS)\\). \\(IRLS\\) is performed by iterating equations  and  until a convergence criterion is not met, such as\r\n\\[\r\n\\sum_{\\ell=1}^{k_{4}} |\\beta_{i}^{(r)} - \\beta_{i}^{(r-1)} |< k_{i} \\times 10^{-4}, \\quad i=1,2, \\ldots, m\r\n\\]\r\nwhere \\(r\\) denotes the algorithm iteration. As starting values of \\(\\boldsymbol{\\beta}_{i}\\) ordinary least squares stimates can be used.\r\nTesting linear and nonlinear restrictions\r\nUnder fairly general conditions \\(ML\\) estimators are asimptotically normally distributed, that is\r\n\\[\r\n\\hat{\\beta}_{ML} \\rightarrow N(\\boldsymbol{\\beta},\\left[\\mathbf{G}'\\left(\\Sigma^{-1} \\otimes I_{T}\\right) G \\right]^{-1})\r\n\\]\r\nTherefore, in order to test restrictions on the parameters of the form\r\n\\[\r\n\\begin{array}{l}\r\nH_{0}: h(\\beta)=0 \\\\\r\nH_{1}: h(\\beta) \\neq 0\r\n\\end{array}\r\n\\]\r\nwe can adopt Wald procedure leading to\r\n\\[\r\nW =h (\\widetilde{\\beta})^{\\prime}\\left[H(\\widetilde{\\beta}) \\widehat{Cov}(\\widetilde{\\beta}) H'(\\tilde{\\beta})\\right]^{-1} h(\\tilde{\\beta})\r\n\\] where \\(\\widehat{Cov}(\\widetilde{\\beta}) = \\left[G'\\left(\\Sigma^{-1} \\otimes I_{T}\\right) G\\right]^{-1}\\). \\(W\\) has a Chi square distribution with \\(r\\) degrees of freedom where \\(r\\) is equal to the rank of the first order derivatives matrix of \\(h\\)\r\nTesting wether \\(\\Omega\\) is diagonal\r\nSuppose we want to test the hypothesis that the errors from the equations are uncorrelated against the alternative that one or more of the off-diagonal elements of \\(\\Sigma\\) are non-zero. That is\r\n\\[\r\n\\begin{array}{l}\r\nH_{0}: \\Sigma = diag(\\sigma_{ii}) \\\\\r\nH_{1}: \\Sigma \\neq diag(\\sigma_{ii})\r\n\\end{array}\r\n\\]\r\nThe log-likelihood ratio statistic which is given by\r\n\\[\r\nLR=2\\left[\\ell(\\tilde{\\theta})-\\sum_{i=1}^{m} \\ell_{i}\\left(\\hat{\\theta}_{i,OLS}\\right)\\right]\r\n\\] Under \\(\\mathbf{H}_{0}\\), \\(LR\\) is asymptotically distributed as a \\(\\chi^{2}\\) with \\(m(m − 1)/2\\) degrees of freedom.\r\nCategory sales sensitivity to price\r\nIn this example we are going to use SURE model to estimate the demand of different product categories in funtion of the average price. Once the system of equations is estimated we can obtain the price elasticities of the different categories. A simpler procdure consists in fitting a separate linear regression to every category and then compute the price elasticities. However, it is reasonable to assume that the different categories are affected by some common drivers and therefore a correlation is observed in the demand equations. For this reason first we fit the SURE model, and next we test the presence of correlation among the equation by testing for the diagonlaity of \\(\\Omega\\). Through the example we will use the systemfit package wich provide a common framework for fitting system of equation models.\r\nWe start the analysis loading the packages needed for the analysis. The data is already loaded in R and is freely available from kaggle.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-01-02T23:35:27+01:00",
    "input_file": "SURE.utf8.md"
  }
]
