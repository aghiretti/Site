[
  {
    "path": "posts/11-01-2020-Co2StateSpace/",
    "title": "CO2 emissions. An analysis using Dynamic Factor Models",
    "description": "In this post we will use DFM to investigate  \nthe CO2 emissions produced over the last 100 years by 15 countries of different geographical and\neconomical areas. To estimate the models we will use the R package MARSS.",
    "author": [
      {
        "name": "Alessandro Ghiretti",
        "url": {}
      }
    ],
    "date": "2021-01-24",
    "categories": [
      "Econometrics",
      "Time series",
      "R",
      "Environment"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nAnalysis\r\nData Description\r\nExploratory Data Analysis\r\nModel specification and estimation\r\nEstimated trends\r\n\r\nForecasts\r\nDiagnostics\r\n\r\nConclusions\r\n\r\n\r\nIntroduction\r\nCarbon dioxide emissions are the primary driver of global climate change and it is widely recognized that to avoid the worst impacts of climate change, the world needs to urgently reduce emissions. Prior to the Industrial Revolution, emissions were very low and growth in emissions was still relatively slow until the mid-20th century. For example, in 1950 the world emitted just over 5 billion tonnes of (CO2), about the same as the US, or half of China’s annual emissions today. By 1990 this had quadrupled to 22 billion tonnes. Emissions have continued to grow rapidly and we now emit over 36 billion tonnes each year. Emissions growth has slowed over the last few years, but they have yet to reach their peak.\r\nThe purpose of this post is to use dynamic factor models and try to identify hidden common trends between the three main geographical areas of EU, North America and Asia that can be used to describe the evolution of the CO2 emissions. We will consider CO2 emissions from 15 of the main Global Economies.To fit our model we will use the R package MARSS\r\nAnalysis\r\nData Description\r\nThe data that we consider available at the following link:\r\nhttps://raw.githubusercontent.com/owid/co2-data/master/owid-co2-data.csv\r\nThe data contains the CO2 emissions, the GPD and other relevant variables for all the world countries starting from 1715 to 1018. In our analysis we will consider data starting from 1900 to 2018 for the following countries: Italy, United Kingdom, France, Germany, Spain, United States, Canada, Japan, India, China, Argentina, Brazil, Peru, New Zealand and Australia\r\n\r\n\r\n\r\nExploratory Data Analysis\r\nWe start the analysis by loading the data into R. In my case the data in in the project folder\r\n\r\n\r\n# Read data\r\nco2 <- read.csv(\"owid-co2-data.csv\")\r\n\r\n\r\n\r\nWe proceed filtering out the countries and the variables that are not of interest to us. Moreover, as it might be interesting to compare the results at continent level we add the variable zone which contains the geographic zone of every country that we selected.\r\n\r\n\r\n# Filter countries to include in the analysis\r\ncountries <- c(\"Italy\",\"United Kingdom\",\"France\",\"Germany\",\"Spain\",\"United States\", \r\n               \"Canada\",\"Japan\", \"India\", \"China\",  \"Argentina\",\r\n               \"Brazil\", \"Peru\", \"New Zealand\", \"Australia\")\r\n\r\n# Define geographical zones \r\nzone <- c(rep(\"Europe\",5), rep(\"North America\", 2), rep(\"Asia\",2),\r\n          rep(\"South America\",3), rep(\"Oceania\",2))\r\n\r\n# Add geographical zones to the data\r\nco2 <-co2 %>% \r\n  mutate(zone = ifelse(country %in% \r\n    c(\"Italy\",\"United Kingdom\",\"France\",\"Germany\",\"Spain\"),\r\n    \"Europe\",ifelse( country %in% c (\"United States\",\"Canada\"), \"North America\",\r\n    ifelse(country %in% c(\"Japan\", \"India\", \"China\"), \"Asia\",\r\n    ifelse(country %in% c(\"Argentina\",\"Brazil\", \"Peru\"),\"South America\", \"Oceania\")))))\r\n\r\n# Filter the data \r\ndat <- co2 %>% \r\n  # filter the countries and the year\r\n  filter(country %in% countries &\r\n         year >= 1910) %>% \r\n  # select columns\r\n  select(iso_code, country, year, zone, co2)\r\n\r\n\r\n\r\nNow we are ready to analyze our data. Lets start by\r\nlooking at the temporal structure of the CO2 emissions for the different countries.\r\n\r\n\r\nnb.cols <- length(countries)\r\nmycolor <- mycolors <- colorRampPalette(brewer.pal(9, \"YlGnBu\"))(nb.cols)\r\n# Co2 Emissions Plot\r\n# Plot co2 emissions\r\np1 <- dat %>% ggplot(aes(x = year, y = co2, color = country)) +\r\n  geom_line() +\r\n  scale_color_manual(values = mycolor) +\r\n  ggtitle(\"CO2 emissions time series\")+\r\n  ylab(\"CO2 emissions\") +\r\n  theme_minimal()\r\n\r\nggplotly(p1)  \r\n\r\n\r\npreserve658409248b4807e5\r\n\r\nAs we immediately see there are big differences in the CO2 emissions. As expected USA is the country with the highest CO2 emissions for the entire period, followed by China that shows an impressive positive trend starting from 1975. Germany, Japan and India represent the other four countries with the highest CO2 values. Since there are great differences between the series, in order to reduce the discrepancies it might be useful to take a look at the log transformed data.\r\n\r\npreserve89813672b45aff5e\r\n\r\nWith the log transform we are able to see that there is a upward trend common to all of the countries. By looking at the whole picture in more detail we see that Peru and New Zealand are the countries with the lowest emission. Moreover, an interesting pattern is observed around 1945 where many countries exhibit severe downward spikes.\r\n\r\n\r\n p1 <- dat %>% ggplot(aes(x = zone, y = co2, color = country)) +\r\n  geom_boxplot() +\r\n   scale_color_manual(values = mycolor) +\r\n  ggtitle(\"CO2 concentration boxplot\")+\r\n  ylab(\"CO2 concentration\") +\r\n  theme_minimal()\r\n\r\n p1\r\n\r\n\r\n\r\n\r\nWhen we consider the aggregated data over the entire period the differences in the CO2 emission between countries are even more evident. Oceania and South America are the zone with the lowest and less variable average emissions. In Europe the emissions appear on average pretty low but more variable compared to those of Oceania and South America. Overall, with the exception of USA and China, we observe that the variability and the average value of the CO2 emissions are similiar between the countries that belong to the same geographical zone.\r\n\r\n\r\n p1 <- dat %>% \r\n  group_by(year,zone) %>% summarise(co2 = sum(co2)) %>% \r\n  ggplot() +\r\n  geom_line(aes(x = year, y = co2, color = zone),\r\n            size = 1.2, alpha = 0.6) +\r\n   scale_color_brewer(palette = \"Set2\") +\r\n  ggtitle(\"CO2 total concentration by area\")+\r\n  ylab(\"CO2 concentration\") +\r\n  theme_minimal()\r\n\r\nggplotly(p1)\r\n\r\n\r\npreserve2e90d1fc14598fee\r\n\r\nTo conclude our exploratory analysis we compute and examine the correlation of the CO2 emissions between the different countries. It appears that in general there is a general high positive correlation between the CO2 levels observed in the different countries and apparently this correlation is not strictly related to the geographical area.\r\n\r\n\r\n\r\nModel specification and estimation\r\nHaving explored our data we proceed to specify the model. In this case we will use a dynamic factor model to identify common trends between the CO2 emissions of the different countries. Compared to VAR, dynamic factor models require an inferior number of parameters to be estimated, and therefore they can be applied to high dimensional context. Following Harvey (referenze) we adopt the following Dynamic Factor model (DFM)\r\n\\[\\begin{align}\r\n\r\n\\begin{pmatrix}\r\nx_{1} \\\\\r\nx_{2} \\\\\r\n\\vdots \\\\\r\n\\x_{m}\r\n\\end{pmatrix}_{t}\r\n\r\n&= \r\n\r\n\\begin{pmatrix}\r\n1 & 0 & \\cdots & 0 \\\\\r\n0 & 1 & \\cdots & 0 \\\\\r\n\\vdots& \\cdots & \\ddots &\\cdots & \\vdots \\\\\r\n0 & 0 & \\cdots & 1\r\n\\end{pmatrix}\r\n\\end{align}\\]\r\nAccording to model (ref) \\(m\\) hidden trends, each one described by a random walk and possibly correlated to the other, can describe CO2 emissions of the different countries. The relevance that each state has on the observed series is expressed by the corresponding factor loading. According to the explanatory analysis we fit a mode with four states and an unrestricted covariance matrix. As it is generally suggested when fitting this kind of models the observations are standardized. In the following we reshape our data in a \\(N \\times T\\) matrix and proceed to fit the model\r\n\r\n\r\n# row: country column: year\r\ny <- dat %>% \r\n  select(-c(\"zone\")) %>% \r\n  pivot_wider(names_from = year, values_from = co2) %>% \r\n  arrange(iso_code)\r\n\r\n# convert the data in matrix\r\ny = as.matrix(y[,3:ncol(y)])\r\n\r\n# set parameters for optimizer\r\ncntl.list <- list(minit = 200, maxit = 5000, allow.degen = FALSE)\r\n\r\n#  Define values for grid search\r\n# define covariance structures for the states\r\nR.structure <- \"unconstrained\"\r\nm <- 4\r\n\r\nmodel.list <- list(m = m, R = R.structure)\r\n# fit model to standardized data\r\nmod.me <- MARSS(y, model = model.list, z.score = TRUE, form = \"dfa\",\r\n                control = cntl.list, silent = TRUE)\r\n\r\n\r\n\r\nEstimated trends\r\nOnce we have estimated our model we can extract and plot the states of the system. Since there exhists many possible solutions to model (ref), we apply the varimax rotation of the factors and of the trends. (referenza)\r\n\r\n\r\n# perform rotation of the states before plotting\r\n# get the inverse of the rotation matrix\r\nZ.est <- coef(mod.me, type = \"matrix\")$Z\r\nH.inv <- 1\r\nif (ncol(Z.est) > 1){\r\nH.inv <- varimax(coef(mod.me, type = \"matrix\")$Z)$rotmat}\r\n\r\n# rotate factor loadings\r\nZ.rot <- Z.est %*% H.inv\r\n#extract the rotated loadings and reshape into a matrix\r\nloadings <- t(Z.est) %>% data.frame()\r\ncolnames(loadings) <- unique(dat$country)\r\nloadings$x <- c(\"X1\",\"X2\", \"X3\", \"X4\")\r\n# add zone to the loadings\r\nloadings <- loadings %>% \r\n  pivot_longer(-c(\"x\"), names_to = \"country\", values_to = \"loading\") %>% \r\n  mutate(zone = ifelse(country %in% \r\n    c(\"Italy\",\"United Kingdom\",\"France\",\"Germany\",\"Spain\"),\r\n    \"Europe\",ifelse( country %in% c (\"United States\",\"Canada\"), \"North America\",\r\n    ifelse(country %in% c(\"Japan\", \"India\", \"China\"), \"Asia\",\r\n    ifelse(country %in% c(\"Argentina\",\"Brazil\", \"Peru\"),\"South America\", \"Oceania\")))))\r\n\r\n\r\n# rotate trends and covnert them to a data frame\r\ntrends.rot <- solve(H.inv) %*% mod.me$states\r\ntrends.rot <- t(trends.rot)\r\ntrends.rot <- data.frame(trends.rot)\r\n\r\n# assign column names\r\ncolnames(trends.rot) <- c(\"x1\",\"x2\",\"x3\",\"x4\")\r\n\r\n# add year and reshape\r\ntrends.rot$Year <- seq(from = 1910, to = 2018)\r\ntrends.rot <- trends.rot %>% \r\n  pivot_longer(cols = -c(\"Year\"),\r\n              names_to = \"state\", values_to = \"val\") %>% \r\n  group_by(state) %>% dplyr::arrange(Year, .by_group = TRUE)\r\n\r\n# obtain estimated states\r\ns <- tsSmooth(mod.me)\r\ncolnames(s) <- c(\"state\", \"Year\", \"val\",\"se\")\r\ns$statesRot <- trends.rot$states \r\ns$Year <- rep(seq(from = 1910, to =2018),4)\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 1: Smoothed states\r\n\r\n\r\n\r\n\r\n\r\npreserve1d58624da02d2207\r\n\r\nFigure 2: Loading factors\r\n\r\n\r\n\r\nFigure 1 shows the smoothed states before and after the varimax rotation and Figure 2 the rotated factors loadings of the different series. By looking at the loadings in Figure 2 we immediately see that X1 represent the state more relevant to all the countries. All the loadings associated to it are positive and it describes the common upward trend observed in the exploratory phase. State X3 is characterized by slow negative trend followed by a slow increasing ramp at the end at the period. The largest loadings, in absolute value associated to it are observed for Italy, Japan, France, china and India. To conclude state X3 and X4 describe two opposite behaviors. It is interesting to note that for states X3 and X2 the loadings associated to Italy, India, Germany and France follow the exact opposite behavior of those associated to Japan and China.\r\nForecasts\r\nHaving inspected the common trends it is time to check if the model is truly able to produce accurate forecasts for the series considered. We consider the one step ahead predictions and the associated prediction errors.\r\n\r\n\r\n\r\n\r\n\r\np1 <- y.hat %>% ggplot() +\r\n  geom_point(aes(x = Year, y = y, group = country),color = \"pink\", alpha = 0.7) +\r\n  geom_line(aes(x = Year, y = estimate, group = country), \r\n            size = 1.2, color = \"royalblue\", alpha = 0.7) +\r\n  facet_wrap(~country, ncol = 3) +\r\n  ggtitle(\"One step ahead predictions\") +\r\n  theme_minimal()\r\n\r\nggplotly(p1)\r\n\r\n\r\npreservebe40af6eb5d6cfab\r\n\r\nFrom plot ?? it appears that forecasts produced by the model are pretty accurate. Few pronounced errors are observed for the series of Italy and France in 1921, 1925 and 1945. In order to quantify the accuracy and compare numerically the predictive power of the model for the different countries we compute the following error metrics:\r\nMSE: \\(mean((y - \\hat{y})^{2})\\)\r\nMAE: \\(mean(|y - \\hat{y}|)\\)\r\nMAPE:\\(mean \\left( \\left| \\frac{100( y - \\hat{y})}{y} \\right| \\right)\\)\r\n\r\n\r\n# define the metrics\r\nmse <- function(x){\r\n   m <- mean((x$y - x$estimate)^2)\r\n   return(m)\r\n}\r\n\r\nmae <- function(x){\r\n   m <- mean(abs(x$y - x$estimate))\r\n   return(m)\r\n}\r\n\r\nmape <- function(x){\r\n   m <- mean(abs(((100*(x$y - x$estimate))/x$y)))\r\n   return(m)\r\n}\r\n\r\n# estimate the metrics for the different series\r\nmetrics <- y.hat %>% group_by(country) %>% \r\n  nest() %>% mutate(mse = lapply(data,mse),\r\n                    mae = lapply(data,mae),\r\n                    mape = lapply(data,mape)) %>% \r\nunnest() %>% \r\n  dplyr::select(country,mse,mae,mape) %>% \r\n  arrange(desc(mse), desc(mae), desc(mape)) %>% \r\n  unique()\r\n\r\nmetrics %>% formattable(list(`mae` = color_bar(\"#FA614B\"),\r\n                             `mse` = color_bar(\"#FA614B\"),\r\n                             `mape` = color_bar(\"#FA614B\")))\r\n\r\n\r\n\r\ncountry\r\n\r\n\r\nmse\r\n\r\n\r\nmae\r\n\r\n\r\nmape\r\n\r\n\r\nItaly\r\n\r\n\r\n0.23449772\r\n\r\n\r\n0.34920901\r\n\r\n\r\n61.627063\r\n\r\n\r\nFrance\r\n\r\n\r\n0.11989005\r\n\r\n\r\n0.25547754\r\n\r\n\r\n55.527271\r\n\r\n\r\nIndia\r\n\r\n\r\n0.06942096\r\n\r\n\r\n0.19600775\r\n\r\n\r\n26.820661\r\n\r\n\r\nUnited States\r\n\r\n\r\n0.03895182\r\n\r\n\r\n0.13126001\r\n\r\n\r\n19.397801\r\n\r\n\r\nUnited Kingdom\r\n\r\n\r\n0.03879142\r\n\r\n\r\n0.11797902\r\n\r\n\r\n18.352430\r\n\r\n\r\nGermany\r\n\r\n\r\n0.02432059\r\n\r\n\r\n0.10238886\r\n\r\n\r\n32.273276\r\n\r\n\r\nCanada\r\n\r\n\r\n0.02263689\r\n\r\n\r\n0.08088415\r\n\r\n\r\n12.099801\r\n\r\n\r\nPeru\r\n\r\n\r\n0.01909765\r\n\r\n\r\n0.07754633\r\n\r\n\r\n10.742386\r\n\r\n\r\nSpain\r\n\r\n\r\n0.01890602\r\n\r\n\r\n0.08433286\r\n\r\n\r\n35.637270\r\n\r\n\r\nNew Zealand\r\n\r\n\r\n0.01766297\r\n\r\n\r\n0.08086102\r\n\r\n\r\n16.778824\r\n\r\n\r\nArgentina\r\n\r\n\r\n0.01546800\r\n\r\n\r\n0.06652019\r\n\r\n\r\n10.597381\r\n\r\n\r\nBrazil\r\n\r\n\r\n0.01469260\r\n\r\n\r\n0.07118643\r\n\r\n\r\n13.188483\r\n\r\n\r\nJapan\r\n\r\n\r\n0.01302614\r\n\r\n\r\n0.07027200\r\n\r\n\r\n12.687478\r\n\r\n\r\nAustralia\r\n\r\n\r\n0.01213309\r\n\r\n\r\n0.04829242\r\n\r\n\r\n9.977388\r\n\r\n\r\nChina\r\n\r\n\r\n0.01097965\r\n\r\n\r\n0.05938297\r\n\r\n\r\n21.672444\r\n\r\n\r\nThere is a general agreement in the metrics for Italy, France and India that appear as the countries for which the model produces the worst predictions. MAjor disagreements are observed for China, Spain and Germany. While according to mse and mae the performances in predicting the CO2 emissions of these countries are not that bad, the same is not observed in the mape. Overall by taking into account that no covariates are included in the model and that the states are modeled as random walk the one step ahead predictions appear good. However, it is important to note that in order to produce accurate forecasts on longer horizons model the states as autoregressive process or including covariates should be seriously considered.\r\nDiagnostics\r\nWe conclude the analysis with the inspection of the residuals plot in order to check if the underlying model assumptions are violated.\r\n\r\n\r\n\r\n\r\n\r\n# plot model residuals\r\np1 <- mod.res %>% ggplot() +\r\n  geom_point(aes(x = Year, y = .resids, group = country), color = \"royalblue\", alpha = 0.4) +\r\n  geom_ribbon(aes(x = Year, ymin = - 1.96*.sigma, \r\n                  ymax = +1.96*.sigma), alpha = 0.4) +\r\n  facet_wrap(~country, ncol = 3) +\r\n  theme_minimal() +\r\n  ylab(\"residuals\") + ggtitle(\"Fitted residuals\")\r\n\r\nggplotly(p1)\r\n\r\n\r\n\r\npreservecfab9417e7c6d6cf\r\n\r\nFigure 3: Fitted residuals\r\n\r\n\r\n\r\n\r\n\r\npreservecfdd838ce8685e61\r\n\r\nFigure 4: State residuals\r\n\r\n\r\n\r\nFrom @(fig:fittedres) we see that the overall fit of the model is not too bad, but for some countries such as Italy, France and India the model is not able to capture a correlation structure that remains evident in the cyclical pattern of the residuals. Moreover, some outlying units appear to be present in the data. This is particularly evident for Italy, France, India and United Kingom. From figure @(fig:stateres) the state residuals appear well dispersed but for state X4 there is some autocorrelation evident from the cyclical pattern.\r\n\r\npreservea42e6c3949ad46b8\r\n\r\n\r\npreserve362e7920ec223feb\r\n\r\nOverall the normality assumption appears satisfied for the fitted residuals, while in the state residuals a slight deviation is observed in state X3 and X4.\r\nConclusions\r\nIn this post we have investigated the application of dynamic factor model to study the CO2 emissions of 15 countries. Despite the model performed well when the forecast horizon is equal to one year, in order to produce forecasts for longer horizons the inclusion of covariates or the use of autoregressive process to describe the evolution of the states is needed. This is also confirmed from the analysis of the residuals that both at observation and state level showed a cyclical pattern. Moreover the inclusion of dummy variable to account for structural breaks should be considered in order to improve the validity of the underlying model.\r\n\r\n\r\n\r\n",
    "preview": "posts/11-01-2020-Co2StateSpace/Preview.jpg",
    "last_modified": "2021-01-24T00:27:23+01:00",
    "input_file": "11-01-2020-Co2StateSpace.utf8.md"
  },
  {
    "path": "posts/04-01-2021-MarkovModel/",
    "title": "Latent Markov Models for modeling customer spending dynamics",
    "description": "In this post we will apply Latent Markov Models to model the customers portfolio \ncomposition of a retailer. To fit the models we will use the R package LMest.",
    "author": [
      {
        "name": "Alessandro Ghiretti",
        "url": {}
      }
    ],
    "date": "2021-01-09",
    "categories": [
      "Econometrics",
      "Latent Markov Models",
      "Marketing"
    ],
    "contents": "\r\n\r\nContents\r\nIntroductionLatent Markov Models\r\nModeling customer spending dynamics via Markov ModelsData description\r\nData analysis\r\n\r\nFitting LMM model.Panel LMM with covariates\r\n\r\n\r\n\r\nIntroduction\r\nMarket or customer segmentation refers to the division of customers or potential customers of a given market into homogeneous groups. The idea is that customers belonging to the same group will common characteristics and their response to marketing activities and initiatives will be almost the same. Customer segmentation can be performed in different ways, for example a very basic method is to divide customers into groups on the basis of they spending, LTV or number of visits to a store. Where in the first two cases the aim is to identify groups of customers with different profitability and target them with different campaigns, discounts or promotions.\r\nIn practice other variables than the number of visits or the spending are taken into account, for example socio-economic variables are often included as it is well known that factors such as age, lifestyle and status have a great impact on the customer behavior. Despite the number of groups in which we want to segment our customers is often known a priori it might happen that there exists hidden partitions that we ignore. In this case statistical techniques can be adopted to decide the number of groups that achieve the best homogeneity between members.\r\nOnce customers are aggregated into different groups the interest might be in monitoring the evolution of these groups over time. For example having collected a sample of customers and having divided them into four groups according to their spending the company would like to know:\r\nWhat is the probability that a customer belongs to the a given group?\r\nWhat is the probability that an high spender becomes a low spender?\r\nWhat is the effect of age on these probabilities?\r\nTo answer these kind of questions we will propose an approach based on Latent Markov Models. In the following we will review the theory behind Latent Markov models subsequently apply them to model the customer spending dynamics\r\nLatent Markov Models\r\nModeling customer spending dynamics via Markov Models\r\nIn this section we apply LMM to model the\r\nData description\r\nThe dataset that we consider is The Complete Journey dataset freely available from Dunn Humby. According to the description of the data provider the dataset contains household level transactions over two years from a group of 2,500 households who are frequent shoppers at a retailer. It contains all of each household’s purchases, not just those from a limited number of categories. For certain households, demographic information as well as direct marketing contact history are included.\r\nData analysis\r\n\r\n\r\n\r\nWe start by importing the data into R into two data.frame the demo which contains the demographical information of the customers and the transactions which contains the transactions performed by every customer in the retail in the last two years. We use janitor to clean and standardize the name of the columns.\r\n\r\n\r\n# clean column names\r\ncolnames(transactions) <- make_clean_names(colnames(transactions))\r\ncolnames(demo) <- make_clean_names(colnames(demo))\r\n\r\n\r\n\r\nSince we are dealing with temporal data the most natural thing is to retain the information provided by the time dimension and apply a longitudinal Latent Markov Model. Therefore We proceed by constructing a panel. To do so we divide the data into quarters each one composed of 120 days for a total of four quarters per year.\r\n\r\n\r\n# build the variable indicating the quarter\r\nquarter <- rep(seq(1:8), each = 120)\r\nday <- seq(1:length(quarter))\r\ntime_quarter <- data.frame(quarter,day)\r\n\r\n# assign the transactions to the corresponding quarter by joining on the day\r\nadjust_transactions <- transactions %>% left_join(time_quarter, by = c(\"day\" = \"day\"))\r\n\r\n\r\n\r\nNext for each customer we obtain the total spending per quarter which will be used to score the customer profitability\r\n\r\n\r\ncustomer_quarter <- adjust_transactions %>% \r\n  # join with demographics data\r\n  left_join(demo, by = c(\"household_key\" = \"household_key\")) %>% \r\n  # group by customer Id\r\n  group_by(household_key, quarter) %>% \r\n  # obtain total spend per customer\r\n  summarise(tot_spend = sum(sales_value)) %>% \r\n  ungroup()\r\n\r\n\r\n\r\nIn order assign a score to every customer we compare total spending in each quarter with the quartile of the quarterly total spending distribution. Next, we assign a score \\(S\\) between 1 and 4, where the two extreme values denote respectively low spenders and high spender customers. In particular we have\r\nLow spenders, \\(S=1\\)\r\n\r\nMiddle-low spenders, \\(S=2\\)\r\n\r\nMiddle-high spenders, \\(S=3\\)\r\n\r\nHigh spenders: \\(S = 4\\),\r\n\r\nwhit score function \\[\r\n\\begin{aligned}\r\nS = \\begin{cases}\r\n& 1 \\quad \\text{if}, \\; I(spending_{iq} \\leq Q_{1}(spending_{q})) = 1  \\\\\r\n& 2 \\quad \\text{if}, \\; I(Q_{1}(spending_{q}) < spending_{iq} \\leq Q_{2}(spending_{q})) = 1 \\\\\r\n& 3 \\quad \\text{if}, \\; I(Q_{2}(spending_{q}) < spending_{iq} \\leq Q_{3}(spending_{q})) = 1 \\\\\r\n& 4 \\quad \\text{if}, \\; I(spending_{iq} > Q_{3}(spending_{q})) = 1  \r\n\\end{cases}\r\n\\end{aligned}\r\n\\]\r\nwhere \\(spending_{iq}\\) denotes the spending of the \\(i\\)th customer in quarter \\(q\\), \\(Q_{j}(spending_{q})\\) denote the \\(j\\)th quartile of the spending distribution in quarter \\(q\\) and \\(I(\\cdot)\\) is the indicator function which results. The next chunk of code compute the quartiles and assign the scores to each customer.\r\n\r\n\r\n# compute quartiles\r\nquantiles <- customer_quarter %>% \r\n  group_by(quarter) %>% \r\n     summarise(q1 = quantile(tot_spend,0.25),\r\n               q2 = quantile(tot_spend,0.5),\r\n               q3 = quantile(tot_spend,0.75))\r\n\r\n\r\n\r\n# assign score to each customer\r\n# in this loop scores are assigned to each customer for each quarter\r\ns <- c()\r\nk <- c()\r\nq <- c()\r\nscore <- s\r\nkey <- k\r\nquarter <- q\r\n\r\nfor(j in seq(1:6)){\r\n  ll <- customer_quarter %>% filter(quarter == j)\r\n  qq <- quantiles %>% filter(quarter == j)\r\n  \r\nfor(i in 1:nrow(ll)){\r\n  k[i] <- ll$household_key[i]\r\n  q[i] <- j\r\n  if(ll$tot_spend[i] <= qq[2]){\r\n    s[i] <- 1\r\n    }else if(ll$tot_spend[i] > qq[2] & ll$tot_spend[i] <= qq[3]){\r\n      s[i] <- 2\r\n        }else if(ll$tot_spend[i] > qq[3] & ll$tot_spend[i] <= qq[4]){\r\n        s[i] <- 3}\r\n      else if(ll$tot_spend[i] > qq[4]) {s[i] <- 4}\r\n}\r\n  \r\nquarter <- c(quarter,q)\r\nscore <- c(score,s)\r\nkey <- c(key,k)\r\n}\r\n\r\n# obtain the scores given to the customers over the different quarters\r\nquarter_scores <- data.frame(household_key = key, score,  quarter)\r\n\r\n# bind scores to the customers spend\r\ncustomer_quarter <- customer_quarter %>% \r\n  left_join(quarter_scores, by = c(\"household_key\" = \"household_key\", \"quarter\" = \"quarter\")) %>% \r\n  dplyr::select(-c(\"tot_spend\")) %>% data.frame()\r\n\r\n\r\n\r\nHaving assigned the score to each customer we proceed to join the results with the socio-economic variables and prepare the data for model fitting. In particular we remove the observations with missing data, convert the variables to factors and consider only the customers that performed a purchase at least once every quarter. This last adjustment is performed in order to have a balanced panel.\r\n\r\n\r\n# bind demographic info and remove customers with missing values\r\ndata_fit <- customer_quarter %>% \r\n  # join tables\r\n  left_join(demo, by = c(\"household_key\")) %>% \r\n  # remove households with missing values\r\n  drop_na()\r\n\r\n# rename variables for model fitting\r\nc_name <- colnames(data_fit)\r\nnc <- ncol(data_fit)\r\nc_name[3] <- paste(\"Y\",sep =\"\",colnames(data_fit)[3])\r\nc_name[4:nc] <- paste(paste(\"X\",seq(1:(nc-3)),sep=\"\"),colnames(data_fit)[4:nc],sep = \"\")\r\ncolnames(data_fit) <- c_name\r\n\r\n# convert explanatory variables into factors\r\nresponse <-  colnames(data_fit)[4:nc]\r\ndata_fit[response] <- lapply(data_fit[response], factor)\r\n\r\n# extract customers which have at least one expense in each quarter\r\nidx = data_fit %>% group_by(household_key) %>% \r\n  # obtain number of quarters per household\r\n  summarise(n = n()) %>% ungroup() %>%  \r\n  # filter out households\r\n  filter(n==6) %>% \r\n  # select  hoousehold key and transform it to a vector\r\n  dplyr::select(household_key) %>% as.vector()\r\n\r\ndata_fit <- data_fit %>% filter(household_key %in% idx$household_key)\r\n\r\n\r\n\r\nAfter the cleaning the data comprises 801 customers each of them observed for one quarter.\r\nFitting LMM model.\r\nIn order to fit the LMM model we will use the LMest ([LMest]) package, developed by F. Bartolucci, F. Pennoni and .\r\nWe will consider two models:\r\n\\(\\mathcal{M}_{1}\\): time heterogeneous LMM with no covariates. In this case no explanatory variables are included in the model but the transition probabilities are allowed to vary in time\r\n\\(\\mathcal{M}_{2}\\): time heterogeneous LMM with covariates. This is an extension of model \\(\\mathcal{M}_{2}\\) where we include socio-economic descriptors are covariates\r\nDespite it is possible to add covariates both in the equation which scribe the evolution of the latent state and in the equation which describe the evolution of the observed series this discouraged as suggested by (Bartolucci, Farcomeni, and Pennoni 2012). The main reasons are that the resulting model might be too complex to interpret and that the number of parameters might increase too much leading to unreliable estimates in the optimization process.\r\nWe proceed do define the formulas for every model and to fit them\r\n\r\n\r\n\r\nWe start by considering the BIC, AIC for M1 and M2\r\n\r\n\r\nmodel\r\n\r\n\r\naic\r\n\r\n\r\nbic\r\n\r\n\r\nM1\r\n\r\n\r\n8275.910\r\n\r\n\r\n8400.253\r\n\r\n\r\nM2\r\n\r\n\r\n8304.931\r\n\r\n\r\n8525.985\r\n\r\n\r\nWe start by comparing the prior probabilities obtained by the two models without covariates. Prior probabilities represents the probability that a customer belongs to a certain class.\r\n\r\n\r\nmodel\r\n\r\n\r\nlow spender\r\n\r\n\r\nmiddle-low spender\r\n\r\n\r\nmiddle-high spender\r\n\r\n\r\nhigh spender\r\n\r\n\r\nM1\r\n\r\n\r\n0.4390471\r\n\r\n\r\n0.006717\r\n\r\n\r\n0.1584249\r\n\r\n\r\n0.3958109\r\n\r\n\r\nM2\r\n\r\n\r\n0.4390471\r\n\r\n\r\n0.006717\r\n\r\n\r\n0.1584249\r\n\r\n\r\n0.3958109\r\n\r\n\r\nThe prior probabilities estimated using from the two models appears pretty different. According to M1 the probability of a customer to belong to the low spenders group is almost 43%, while according to M2 this probability is only 26%. Conversely M2 shows an increase in the estimated prior probabilities associate to the middle-low spender and middle-high spenders groups, nevertheless the prior probability estimated by the two models for the high spender group is almost the same around 40%. To understand the dynamics of the customer segments we can analyze the transition probabilities estimated by the two models.\r\n\r\n\r\n\r\nThe plots represent the Markov diagram and the associated transition probabilities. As model M2 allows for time varying probabilities, the diagram shows the averaged transition probabilities over the 6 quarters. We will focus on the interpretation of the plot describing the M2 dynamics. From the diagram it is evident that high spender customers have the highest probability of remaining in their actual state over the next 4 months. This is a really positive news for the company as it might represent the fact that the company is retaining is high segment properly. The same also holds for middle-high spenders that have an estimated probability of 70% of remaining in the state. However, in this case there in an average estimated probability of 14% that a middle-spender customer will become a middle-low spender over the next quarter. Moreover, the estimated probability of a low spender to increase his spending and become a middle spender is 17% which is really positive since once a customer becomes a middle-low spender the probability of subsequently to state 3 and increase further his spending is equal to 20%.\r\nThe dynamics of the customers between the different ghroups can be investingated by looking ad the marginal distributions over time.\r\n\r\n\r\n\r\nFigure 1: Marginal distributions\r\n\r\n\r\n\r\nFig 1 shows the distribution of the states over time. The high spenders segment shows the highest stability over the 6 quarters. The low spenders segment show a steady decline between the first and the second quarter with an increase in the number of subjects belonging to the middle groups. there is an increase in the number of customers belonging to the middle spenders groups with a slight decline between the fifth and sixth quarter.\r\nPanel LMM with covariates\r\nWe conclude the analysis discussing the panel Latent Markov model fitted including the explanatory variables.\r\n\r\n\r\n\r\nBartolucci, Francesco, Alessio Farcomeni, and Fulvia Pennoni. 2010. “An Overview of Latent Markov Models for Longitudinal Categorical Data.” arXiv Preprint arXiv:1003.2804.\r\n\r\n\r\n———. 2012. Latent Markov Models for Longitudinal Data. CRC Press.\r\n\r\n\r\nBartolucci, Francesco, Silvia Pandolfi, and Fulvia Pennoni. 2017. “LMest: An R Package for Latent Markov Models for Longitudinal Categorical Data.” Journal of Statistical Software 81 (4): 1–38. https://doi.org/10.18637/jss.v081.i04.\r\n\r\n\r\nWoodside, Arch G., and Randolph J. Trappey. 1996. “Customer Portfolio Analysis Among Competing Retail Stores.” Journal of Business Research 35 (3): 189–200. https://doi.org/https://doi.org/10.1016/0148-2963(95)00124-7.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/04-01-2021-MarkovModel/04-01-2021-MarkovModel_files/figure-html5/figures-side-1.png",
    "last_modified": "2021-01-09T16:36:04+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/18-12-2020-DynamicRegression/",
    "title": "Media Mix Modeling via Dynamic Linear Regression",
    "description": "In this post we discuss the application of the Dynamic Linear Regression \nmodel to Media Mix Modeling. To fit the model we will use the R package MARSS.",
    "author": [
      {
        "name": "Alessandro Ghiretti",
        "url": {}
      }
    ],
    "date": "2020-08-12",
    "categories": [
      "Econometrics",
      "R",
      "Time Series",
      "Marketing"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nDynamic Linear Model\r\nDynamic regression with random walk cofficients\r\nDynamic regression with AR(1) coefficients\r\n\r\nDynamic Marketing Mix Model\r\nData description\r\nPreliminary Analysis\r\nDynamic linear regression\r\nModel analysis\r\nModel diagnostic\r\n\r\n\r\n\r\nIntroduction\r\nMarketers are generally interested in building models that link different marketing variables such as sales activities, operations and external factors, to changes in consumer behavior, such as acquisition, sales, revenue, and retention. Once the model are implemented these can then support the development of forwardlooking business simulations and optimization exercises. When only advertising variables are included in the model this practice is generally called media mix modeling.\r\nOne of the models most used in practice for media mix modeling is the linear regression model. Once the regression is fitted and unknown parameters are estimated, it is possible to infer the marginal effect that the different explanatory variables have on the response variable. One limitation of the plain linear regression model is that the estimates of the parameters remain constant over time. This assumption might be limited in some context, for example in marketing, where it is well known that the preferences of the customers or the penetration of a certain media evolves with time.\r\nIn order to overcome this limitation the Dynamic Linear Regression Model (DLM), which allows for time varying parameters represent a powerful alternative to plain linear regression. Once the DLM is formulated in State Space form it is possible to estimate the parameters in a dynamic way and conduct a series of inferential procedures.\r\nDynamic Linear Model\r\nA Dynamic Linear Regression (DLR) model is a linear regression model where the parameters are allowed to change over time according to a specific stochastic process. A natural representation of the DLM is the state space form, which represent a general representation for many other statistical models. In its most general form a state space model is defined by the following two equations\r\n$$\r\n\\[\\begin{aligned}\r\nx_{t} & =B_{t} x_{t-1}+ u_{t}+ C_{t} c_{t}+ G_{t} w_{t}, & \\quad w_{t} \\sim MVN\\left(0, Q_{t}\\right) \\\\\r\ny_{t} & =Z_{t} x_{t}+a_{t}+D_{t} d_{t}+H_{t} v_{t}, & \\quad v_{t} \\sim MVN\\left(0, R_{t}\\right) \\\\\r\n\r\n\\end{aligned}\\]\r\n$$\r\nwhere \\(x_{1} \\sim MVN(\\pi, \\Lambda)\\) or \\(x_{0} \\sim MVN(\\pi, \\Lambda)\\). Here \\(y\\) and \\(x\\) can be multivariate series and the index \\(t\\) and denotes that the matrix are allowed to change over time. The different matrices are in generally not know and are related to a series of unknown parameters that need to be estimated. Estimation of the unknown parameters and the related inference are obtained by running the Kalman Filter see Durbin and Koopman (2012) or Andrew C. Harvey and Fernandes (1989) for a detailed description.Given a standard linear regression model\r\n\\[\r\ny_{i} = x'_{i} \\beta + \\epsilon_{i} \\quad \\epsilon_{i} \\sim N(0, \\sigma^{2}) \r\n\\]\r\nwhere \\(y_{i}\\) is a scalar variable, \\(x_{i}\\) is a $ k $ vector of regressors, \\(\\beta\\) is a \\(k \\times 1\\) vector of unknown parameters and \\(\\epsilon_{i}\\) is the error term, a dynamic linear model is obtained by allowing the paramters \\(\\beta\\) to evolve over time thorough a prescribed model which describes the evolution of \\(\\beta\\) over time. That is\r\n\\[\r\n\\begin{aligned}\r\ny_{i} & = x'_{i} \\beta_{t} + \\epsilon_{t} \\quad  & \\epsilon_{t} \\sim N(0,\\sigma^{2})\\\\\r\n\\beta_{t}& = f(\\beta_{t-1}) + v_{t} \\quad  & v_{t} \\sim N(0,R)\r\n\\end{aligned}\r\n\\] Several specifications of \\(f(\\cdot)\\) can be specified but here we will consider the cases where \\(f(\\cdot)\\) is a linear function. Following the state space specification () the dynamic linear regression model can be easily casted in state space form. The unknown parameters becomes the state of the system and the smoothed states obtained with the Kalman Filter represent the estimates of the parameters conditioned on all of the observed data. Depending on the specification of the function \\(f(\\cdot)\\) different models can be identified. The two most simplest model are obtained when a random walk process or an autorgressive process are adopted to describe the evolution of \\(\\beta_{t}\\)\r\nDynamic regression with random walk cofficients\r\nIn this case each \\(\\beta_{t}\\) is assumed to be non stationary and it is modeled as a random walk. That is,\r\n\\[\\begin{aligned}\r\n\r\ny_{t} & = \r\n\r\n\\underbrace{\\begin{pmatrix}\r\n1 & x_{1} & \\cdots & x_{k}\r\n\\end{pmatrix}_{t}}_{B_{t}}\r\n\r\n\\underbrace{\r\n\\begin{pmatrix}\r\n\\beta_{1} \\\\\r\n\\beta_{2} \\\\\r\n\\vdots \\\\\r\n\\beta_{k+1}\r\n\\end{pmatrix}_{t}}_{x_{t}}\r\n\r\n+ w_{t}\r\n\r\n\\\\\r\n\r\n\\underbrace{\\begin{pmatrix}\r\n\\beta_{1} \\\\\r\n\\beta_{2} \\\\\r\n\\vdots \\\\\r\n\\beta_{k+1}\r\n\\end{pmatrix}_{t}}_{x_{t}}\r\n\r\n\r\n& = \r\n\r\n\\underbrace{\\begin{pmatrix}\r\n1 & 0 & \\cdots & 0 \\\\\r\n0 & 1 & \\cdots & 0 \\\\\r\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\r\n0 & 0 & \\cdots & 1 \\\\\r\n\\end{pmatrix}_{t}}_{Z_{t}}\r\n\r\n\\underbrace{\\begin{pmatrix}\r\n\\beta_{1} \\\\\r\n\\beta_{2} \\\\\r\n\\vdots \\\\\r\n\\beta_{k+1}\r\n\\end{pmatrix}_{t-1}}_{x_{t-1}}\r\n\r\n+\r\n\r\n\r\n\\underbrace{\\begin{pmatrix}\r\nv_{1} \\\\\r\nv_{2} \\\\\r\n\\vdots \\\\\r\nv_{k+1}\r\n\\end{pmatrix}_{t}}_{v_{t}}\r\n\r\n\r\n\\end{aligned}\\]\r\nDynamic regression with AR(1) coefficients\r\nWhen a stationarity assumption is necessary on the parameters \\(\\beta_{t}\\) the autoregressive process of order one represent the simplest form of dynamic linear model.\r\n\\[\\begin{aligned}\r\n\r\ny_{t} & = \r\n\r\n\\underbrace{\\begin{pmatrix}\r\n1 & x_{1} & \\cdots & x_{k}\r\n\\end{pmatrix}_{t}}_{B_{t}}\r\n\r\n\\underbrace{\r\n\\begin{pmatrix}\r\n\\beta_{1} \\\\\r\n\\beta_{2} \\\\\r\n\\vdots \\\\\r\n\\beta_{k+1}\r\n\\end{pmatrix}_{t}}_{x_{t}}\r\n\r\n+ w_{t}\r\n\r\n\\\\\r\n\r\n\\underbrace{\\begin{pmatrix}\r\n\\beta_{1} \\\\\r\n\\beta_{2} \\\\\r\n\\vdots \\\\\r\n\\beta_{k+1}\r\n\\end{pmatrix}_{t}}_{x_{t}}\r\n\r\n\r\n& = \r\n\r\n\\underbrace{\\begin{pmatrix}\r\n\\phi_{11} & 0 & \\cdots & 0 \\\\\r\n0 & \\phi_{21} & \\cdots & 0 \\\\\r\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\r\n0 & 0 & \\cdots & \\phi_{k1} \\\\\r\n\\end{pmatrix}_{t}}_{Z_{t}}\r\n\r\n\\underbrace{\\begin{pmatrix}\r\n\\beta_{1} \\\\\r\n\\beta_{2} \\\\\r\n\\vdots \\\\\r\n\\beta_{k+1}\r\n\\end{pmatrix}_{t-1}}_{x_{t-1}}\r\n\r\n+\r\n\r\n\r\n\\underbrace{\\begin{pmatrix}\r\nv_{1} \\\\\r\nv_{2} \\\\\r\n\\vdots \\\\\r\nv_{k+1}\r\n\\end{pmatrix}_{t}}_{v_{t}}\r\n\r\n\\end{aligned}\\]\r\nDynamic Marketing Mix Model\r\nData description\r\nThe data contains sales (expressed in thousand of units) and the corresponding advertising budget (expressed in thousand of dollars) for several media. The data ranges from 2001/01/01 to 2017/08/01 for a total of 200 observations.\r\nPreliminary Analysis\r\nWe begin by loading the packages used for the analysis\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\nlibrary(ggplot2)\r\nlibrary(MARSS)\r\nlibrary(corrplot)\r\nlibrary(patchwork)\r\nlibrary(reshape2)\r\n\r\n\r\n\r\n\r\n\r\n\r\nAfter loading the data that in my case was on a desktop folder we begin the analysis looking at the different time series.\r\n\r\n\r\n  advertising %>%\r\n  select(-c(\"Native\",\"Programmatic\",\"OOH\",\"sales\")) %>% \r\n  pivot_longer(cols = -c(\"Time\"), names_to = \"Media\") %>% \r\n  ggplot(aes(x = Time, y = value, color = Media, )) +\r\n  geom_line() +\r\n  facet_grid(rows  = vars(Media))+\r\n  scale_x_date(date_breaks = \"1 year\",date_labels = \"%Y\") +\r\n  theme(strip.background = element_blank(),\r\n  strip.text = element_blank(), \r\n  axis.text.x = element_text(angle = 90)) +\r\n  scale_y_continuous(breaks = seq(0, 250, by = 125))\r\n\r\n\r\n\r\n\r\nA time series plot reveals that there is a great difference between the budget allocated to the different media. TV represents the media to which more money are allocated and the investments are divided depending on the TV program. Moreover, it emerges clearly that when some media are used other are stopped. In particular overt the period 2009-2011 no budget was allocated to Search and social media but a high investment was allocated to tv_cricket.\r\n\r\n\r\n\r\nFrom the correlation matrix it appears that sales are positively correlated with the budget allocated to TV, Radio, Social, Display rest and Search while there is no apparent linear relationship with the other media. TV and Radio represent the classical media more correlated with sales, while Social, Display_Rest and Search are the modern media with the highest correlation. As it is evident from the correlation matrix there is a strong positive correlation between the budget allocated to different media. For example there is an high positive correlation between Social, tv_sponsorship, Display_Rest and Search, meaning that when the budget in increased in one of these media is increased also in the others.\r\nDynamic linear regression\r\n\r\n\r\n# convert the data into a matrix\r\nresponse <- matrix(advertising$sales, nrow = 1)\r\n\r\n# covariates matrix (k x TT)\r\ncovariates <- advertising %>% select(-c(\"Time\",\"sales\"))\r\ncovariates <- t(covariates) \r\n\r\n# number of state = # of regression params (slope(s) + intercept)\r\nm <- 1 + 12\r\n\r\n# obtain the total number or observations\r\nTT <- length(response)\r\n\r\n\r\n\r\nNext, we proceed to specify the matrices needed for fitting the dlm model\r\n\r\n\r\n# STATE EQUATION\r\n# specify the B matrix \r\nB <- diag(m)\r\n# specify B matrix for AR1 states\r\ndiag(B)[2:13] <- paste(\"b\",2:13, sep = \"\")\r\n# specify remaining matrices\r\nU <- matrix(0, nrow = m, ncol = 1) \r\nQ <- \"unconstrained\"\r\n\r\n# OBSERVATION EQUATION\r\nZ <- array(NA, c(1, m, TT)) # NxMxT; empty for now\r\nZ[1, 1, ] <- rep(1, TT) # Nx1; 1's for intercept\r\nZ[1, 2:13, ] <- covariates # Nx1; regr variable\r\nA <- matrix(0) # 1x1; scalar = 0\r\nR <- matrix(\"r\") # 1x1; scalar = r\r\n\r\n# initialize thate vector\r\ninits.list <- list(x0 = matrix(0, nrow = m))\r\n# set parameters for optimizer\r\ncntl.list <- list(minit = 200, maxit = 20000)\r\n\r\n# list of model matrices & vectors\r\nmod.list <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R)\r\n\r\n\r\n\r\nAnd finally we fit the model.\r\n\r\n\r\n# dlm with autoregressive states\r\ndlr <- MARSS(response, inits = inits.list, model = mod.list,\r\n                 control = cntl.list, silent = TRUE)\r\n\r\n\r\nError in chol.default(denom) : \r\n  il minore principale di ordine 7 non è definito positivo\r\n\r\nModel analysis\r\nWe start by examining the smoothed states in Figure 1.\r\n\r\n\r\n\r\nFigure 1: Smoothed states\r\n\r\n\r\n\r\nLooking at the smoothed states it appears that there is an high variation in the states. The intercept, which represent the average volume of sales in absence of advertising activities in sales shows an increasing trend. Display_Rest, Native, OOH, Programmatic Radio and Magazines shows an increasing marginal effect on the quantity of units sold. Programmatic being the one with the highest marginal effect. The interpretation of the estimated negative values associated to the budget allocated to a particular media has many interpretations. It might indicate that for that period the particular activity performed on that media was not appreciated by the customer, or again it might indicate that the customer got bored or not interested in that media and using it as advertisement produced the opposite effect on sales.\r\n\r\n\r\n\r\nThe correlation structure of the states is of complex interpretation. However the important thing is that it highlights the fact that there exhists synergies between some medias. For example when the marginal effect of the budget allocated to tv_sponsorship increases we expect that also the marginal effect allocated to budget will increase. Apart from the theoretical assumptions on the regression parameters one of the main advantages in adopting a state space representation is the possibility of producing forecasts as soon as new data is available. By running the Kalman filter, it is in fact possible to produce the one step ahead forecasts, \\(\\hat{y}_{t+1|t}\\), in an iterative fashion. The one step ahead predictions obtained with the fitted dynamic linear regression are reported in Figure ??.\r\n\r\n\r\n\r\nModel diagnostic\r\nWe conclude the analysis by inspecting the residuals plot, in order to identify violation to the underlying model assumptions\r\n\r\nMARSSresiduals.tT reported warnings. See msg element or attribute of returned residuals object.\r\nMARSSresiduals.tT reported warnings. See msg element or attribute of returned residuals object.\r\n\r\n\r\n\r\n\r\nFigure 2: Fitted residuals\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 3: State residuals\r\n\r\n\r\n\r\nBoth the model and the state residuals appear to be well dispersed with no particular patterns that may indicate autocorrelation or cyclical behavior. Both in the model and in the state residuals plot some outliers can be identified.\r\n\r\n\r\n\r\n\r\n\r\n\r\nFinally, the inspection of the qqplots reveal that there is no clear departure from normality in the state while the model residuals appear to have heavier tails than a normal distribution.\r\n\r\n\r\n\r\nChan, David, and Mike Perry. 2017. “Challenges and Opportunities in Media Mix Modeling.”\r\n\r\n\r\nDurbin, James, and Siem Jan Koopman. 2012. Time Series Analysis by State Space Methods. Oxford university press.\r\n\r\n\r\nHarvey, A. C., and G. D. A. Phillips. 1982. “The Estimation of Regression Models with Time-Varying Parameters.” In Games, Economic Dynamics, and Time Series Analysis, edited by M. Deistler, E. Fürst, and G. Schwödiauer, 306–21. Heidelberg: Physica-Verlag HD.\r\n\r\n\r\nHarvey, Andrew C, and Clara Fernandes. 1989. “Time Series Models for Count or Qualitative Observations.” Journal of Business & Economic Statistics 7 (4): 407–17.\r\n\r\n\r\nHolmes, Elizabeth, Eric Ward, Mark Scheuerell, and Kellie Wills. 2020. MARSS: Multivariate Autoregressive State-Space Modeling. https://CRAN.R-project.org/package=MARSS.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/18-12-2020-DynamicRegression/Preview.jpg",
    "last_modified": "2021-01-31T22:04:55+01:00",
    "input_file": "18-12-2020-DynamicRegression.utf8.md"
  }
]
