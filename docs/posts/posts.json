[
  {
    "path": "posts/04-01-2021-MarkovModel/",
    "title": "Latent Markov Models for modeling customer spending dynamics",
    "description": "In this post we will apply Latent Markov Models to model the customers portfolio \ncomposition of a retailer. To fit the models we will use the R package LMest",
    "author": [
      {
        "name": "Alessandro",
        "url": {}
      },
      {
        "name": "",
        "url": {}
      }
    ],
    "date": "2021-01-08",
    "categories": [
      "Econometrics",
      "Latent Markov Models",
      "Marketing"
    ],
    "contents": "\r\n\r\nContents\r\nIntroductionLatent Markov Models\r\nModeling customer spending dynamics via Markov ModelsData description\r\nData analysis\r\n\r\nFitting LMM model.\r\n\r\n\r\nIntroduction\r\nMarket or customer segmentation refers to the division of customers or potential customers of a given market into homogeneous groups. The idea is that customers belonging to the same group will common characteristics and their response to marketing activities and initiatives will be almost the same. Customer segmentation can be performed in different ways, for example a very basic method is to divide customers into groups on the basis of they spending, LTV or number of visits to a store. Where in the first two cases the aim is to identify groups of customers with different profitability and target them with different campaigns, discounts or promotions.\r\nIn practice other variables than the number of visits or the spending are taken into account, for example socio-economic variables are often included as it is well known that factors such as age, lifestyle and status have a great impact on the customer behavior. Despite the number of groups in which we want to segment our customers is often known a priori it might happen that there exists hidden partitions that we ignore. In this case statistical techniques can be adopted to decide the number of groups that achieve the best homogeneity between members.\r\nOnce customers are aggregated into different groups the interest might be in monitoring the evolution of these groups over time. For example having collected a sample of customers and having divided them into four groups according to their spending the company would like to know:\r\nWhat is the probability that a customer belongs to the a given group?\r\nWhat is the probability that an high spender becomes a low spender?\r\nWhat is the effect of age on these probabilities?\r\nTo answer these kind of questions we will propose an approach based on Latent Markov Models. In the following we will review the theory behind Latent Markov models subsequently apply them to model the customer spending dynamics\r\nLatent Markov Models\r\nModeling customer spending dynamics via Markov Models\r\nIn this section we apply LMM to model the\r\nData description\r\nThe dataset that we consider is The Complete Journey dataset freely available from Dunn Humby. According to the description of the data provider the dataset contains household level transactions over two years from a group of 2,500 households who are frequent shoppers at a retailer. It contains all of each household’s purchases, not just those from a limited number of categories. For certain households, demographic information as well as direct marketing contact history are included.\r\nData analysis\r\n\r\n\r\n\r\nWe start by importing the data into R into two data.frame the demo which contains the demographical information of the customers and the transactions which contains the transactions performed by every customer in the retail in the last two years. We use janitor to clean and standardize the name of the columns.\r\n\r\n\r\n# clean column names\r\ncolnames(transactions) <- make_clean_names(colnames(transactions))\r\ncolnames(demo) <- make_clean_names(colnames(demo))\r\n\r\n\r\n\r\nSince we are dealing with temporal data the most natural thing is to retain the information provided by the time dimension and apply a longitudinal Latent Markov Model. Therefore We proceed by constructing a panel. To do so we divide the data into quarters each one composed of 120 days for a total of four quarters per year.\r\n\r\n\r\n# build the variable indicating the quarter\r\nquarter <- rep(seq(1:8), each = 120)\r\nday <- seq(1:length(quarter))\r\ntime_quarter <- data.frame(quarter,day)\r\n\r\n# assign the transactions to the corresponding quarter by joining on the day\r\nadjust_transactions <- transactions %>% left_join(time_quarter, by = c(\"day\" = \"day\"))\r\n\r\n\r\n\r\nNext for each customer we obtain the total spending per quarter which will be used to score the customer profitability\r\n\r\n\r\ncustomer_quarter <- adjust_transactions %>% \r\n  # join with demographics data\r\n  left_join(demo, by = c(\"household_key\" = \"household_key\")) %>% \r\n  # group by customer Id\r\n  group_by(household_key, quarter) %>% \r\n  # obtain total spend per customer\r\n  summarise(tot_spend = sum(sales_value)) %>% \r\n  ungroup()\r\n\r\n\r\n\r\nIn order assign a score to every customer we compare total spending in each quarter with the quartile of the quarterly total spending distribution. Next, we assign a score \\(S\\) between 1 and 4, where the two extreme values denote respectively low spenders and high spender customers. In particular we have\r\nLow spenders, \\(S=1\\)\r\n\r\nMiddle-low spenders, \\(S=2\\)\r\n\r\nMiddle-high spenders, \\(S=3\\)\r\n\r\nHigh spenders: \\(S = 4\\),\r\n\r\nwhit score function \\[\r\n\\begin{aligned}\r\nS = \\begin{cases}\r\n& 1 \\quad \\text{if}, \\; I(spending_{iq} \\leq Q_{1}(spending_{q})) = 1  \\\\\r\n& 2 \\quad \\text{if}, \\; I(Q_{1}(spending_{q}) < spending_{iq} \\leq Q_{2}(spending_{q})) = 1 \\\\\r\n& 3 \\quad \\text{if}, \\; I(Q_{2}(spending_{q}) < spending_{iq} \\leq Q_{3}(spending_{q})) = 1 \\\\\r\n& 4 \\quad \\text{if}, \\; I(spending_{iq} > Q_{3}(spending_{q})) = 1  \r\n\\end{cases}\r\n\\end{aligned}\r\n\\]\r\nwhere \\(spending_{iq}\\) denotes the spending of the \\(i\\)th customer in quarter \\(q\\), \\(Q_{j}(spending_{q})\\) denote the \\(j\\)th quartile of the spending distribution in quarter \\(q\\) and \\(I(\\cdot)\\) is the indicator function which results. The next chunk of code compute the quartiles and assign the scores to each customer.\r\n\r\n\r\n# compute quartiles\r\nquantiles <- customer_quarter %>% \r\n  group_by(quarter) %>% \r\n     summarise(q1 = quantile(tot_spend,0.25),\r\n               q2 = quantile(tot_spend,0.5),\r\n               q3 = quantile(tot_spend,0.75))\r\n\r\n\r\n\r\n# assign score to each customer\r\n# in this loop scores are assigned to each customer for each quarter\r\ns <- c()\r\nk <- c()\r\nq <- c()\r\nscore <- s\r\nkey <- k\r\nquarter <- q\r\n\r\nfor(j in seq(1:6)){\r\n  ll <- customer_quarter %>% filter(quarter == j)\r\n  qq <- quantiles %>% filter(quarter == j)\r\n  \r\nfor(i in 1:nrow(ll)){\r\n  k[i] <- ll$household_key[i]\r\n  q[i] <- j\r\n  if(ll$tot_spend[i] <= qq[2]){\r\n    s[i] <- 1\r\n    }else if(ll$tot_spend[i] > qq[2] & ll$tot_spend[i] <= qq[3]){\r\n      s[i] <- 2\r\n        }else if(ll$tot_spend[i] > qq[3] & ll$tot_spend[i] <= qq[4]){\r\n        s[i] <- 3}\r\n      else if(ll$tot_spend[i] > qq[4]) {s[i] <- 4}\r\n}\r\n  \r\nquarter <- c(quarter,q)\r\nscore <- c(score,s)\r\nkey <- c(key,k)\r\n}\r\n\r\n# obtain the scores given to the customers over the different quarters\r\nquarter_scores <- data.frame(household_key = key, score,  quarter)\r\n\r\n# bind scores to the customers spend\r\ncustomer_quarter <- customer_quarter %>% \r\n  left_join(quarter_scores, by = c(\"household_key\" = \"household_key\", \"quarter\" = \"quarter\")) %>% \r\n  dplyr::select(-c(\"tot_spend\")) %>% data.frame()\r\n\r\n\r\n\r\nHaving assigned the score to each customer we proceed to join the results with the socio-economic variables and prepare the data for model fitting. In particular we remove the observations with missing data, convert the variables to factors and consider only the customers that performed a purchase at least once every quarter. This last adjustment is performed in order to have a balanced panel.\r\n\r\n\r\n# bind demographic info and remove customers with missing values\r\ndata_fit <- customer_quarter %>% \r\n  # join tables\r\n  left_join(demo, by = c(\"household_key\")) %>% \r\n  # remove households with missing values\r\n  drop_na()\r\n\r\n# rename variables for model fitting\r\nc_name <- colnames(data_fit)\r\nnc <- ncol(data_fit)\r\nc_name[3] <- paste(\"Y\",sep =\"\",colnames(data_fit)[3])\r\nc_name[4:nc] <- paste(paste(\"X\",seq(1:(nc-3)),sep=\"\"),colnames(data_fit)[4:nc],sep = \"\")\r\ncolnames(data_fit) <- c_name\r\n\r\n# convert cexplanatory variables int factors\r\nresponse <-  colnames(data_fit)[4:nc]\r\ndata_fit[response] <- lapply(data_fit[response], factor)\r\n\r\n# extract customers which have at least one expense in each quarter\r\nidx = data_fit %>% group_by(household_key) %>% \r\n  # obtain number of quarters per household\r\n  summarise(n = n()) %>% ungroup() %>%  \r\n  # filter out households\r\n  filter(n==6) %>% \r\n  # select  hoousehold key and transform it to a vector\r\n  dplyr::select(household_key) %>% as.vector()\r\n\r\ndata_fit <- data_fit %>% filter(household_key %in% idx$household_key)\r\n\r\n\r\n\r\nAfter the cleaning the data comprises 801 customers each of them observed for one quarter.\r\nFitting LMM model.\r\nIn order to fit the LMM model we will use the LMest ([LMest]) package, developed by F. Bartolucci, F. Pennoni and . We will consider three different models.\r\n\\(\\mathcal{M}_{1}\\): time homogeneous LMM with no covariates. This is the simplest models and transition probabilites are not allowed to change over time.\r\n\\(\\mathcal{M}_{2}\\): time heterogeneous LMM with no covariates. In this case we consider a panel and the transition probabilities are allowed to vary in time\r\n\\(\\mathcal{M}_{3}\\): time heterogeneous LMM with covariates. This is an extension of model \\(\\mathcal{M}_{3}\\) where explanatory variables are included.\r\nDespite it is possible to add covariates both in the equation which scribe the evolution of the latent state and in the equation which describe the evolution of the observed series this discouraged as suggested by (Bartolucci, Farcomeni, and Pennoni 2012). The main reasons are that the resulting model might be too complex to interpret and that the number of parameters might increase too much leading to unreliable estimates in the optimization process.\r\nWe proceed do define the formulas for every model and to fit them\r\n\r\n\r\n\r\n\r\n\r\n\r\nBartolucci, Francesco, Alessio Farcomeni, and Fulvia Pennoni. 2010. “An Overview of Latent Markov Models for Longitudinal Categorical Data.” arXiv Preprint arXiv:1003.2804.\r\n\r\n\r\n———. 2012. Latent Markov Models for Longitudinal Data. CRC Press.\r\n\r\n\r\nBartolucci, Francesco, Silvia Pandolfi, and Fulvia Pennoni. 2017. “LMest: An R Package for Latent Markov Models for Longitudinal Categorical Data.” Journal of Statistical Software 81 (4): 1–38. https://doi.org/10.18637/jss.v081.i04.\r\n\r\n\r\nWoodside, Arch G., and Randolph J. Trappey. 1996. “Customer Portfolio Analysis Among Competing Retail Stores.” Journal of Business Research 35 (3): 189–200. https://doi.org/https://doi.org/10.1016/0148-2963(95)00124-7.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-01-08T10:36:32+01:00",
    "input_file": {}
  },
  {
    "path": "posts/18-12-2020-DynamicRegression/",
    "title": "Media Mix Modeling via Dynamic Linear Regression",
    "description": "In this post we discuss the application of Dynamic Linear Regression \nmodel to Media Mix Modeling. We adopt a state space representation of the \nlinear regression model and use the dlm R package to fit the model to a\nmedia mix modeling example.",
    "author": [
      {
        "name": "Alessandro",
        "url": {}
      },
      {
        "name": "",
        "url": {}
      }
    ],
    "date": "2021-01-08",
    "categories": [
      "Econometrics",
      "State Space Models",
      "Time Series"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nState Space ModelsFiltering, Smoothing and Forecasting\r\nRegression with time varying parameters\r\n\r\nDynamic Marketing Mix ModelData description\r\nPreliminary Analysis\r\nMedia mix model specification\r\nFittinig dynamic models in R: the dlm package\r\nElasticities\r\nConclusions\r\n\r\n\r\nIntroduction\r\nMarketers are generally interested in building models that link different marketing variables such as sales activities, operations and external factors, to changes in consumer behavior, such as acquisition, sales, revenue, and retention .Once the model are implemented these can then support the development of forwardlooking business simulations and optimization exercises. When only advertising variables are included in the model this practice is generally called media mix modeling.\r\nOne of the models most used in practice for media mix modeling is the linear regression model. Once the regression is fitted and unknown parameters are estimated, it is possible to infer the effect that the different explanatory variables have on the response variable and obtain key information such as marginal estimated contributions and elasticities. One of the key assumptions about the plain linear regression model is that the estimates of the parameters remain constant over the period considered. This assumption appears to be limited in many context, for example in marketing it might be assumed that the marginal effect on budget allocated to different media changes, as the customer behavior or the penetration of one media evolves over time.\r\nIn order to overcome this limitation the Dynamic Linear Regression Model (DLM), which allows for time varying parameters represent a powerful alternative to plain linear regression. Once the DLM is casted in State Space form it is possible to estimate the parameters in a dynamic way and conduct a series of inferential procedures.\r\nWe will first give an introduction to the State Space models, and then we will show, using the R package dlm, how to fit a DLM to a Media Mix Modeling problem.\r\nState Space Models\r\nState space models provide a unified methodology for treating a wide range of problems in time series analysis. In this approach it is assumed that the development of the time series under study is determined by an unobserved component \\(\\alpha_{1},\\dots,\\alpha_{n}\\), with which are associated a series of observations \\(y_{1},\\dots,y_{n}\\). The unobserved components are called the states of the process and their evolution is assumed to be described by a first order Markov process. The relation between the \\(\\alpha_{t}\\)’s and the \\(y_{t}\\)’s is specified by the state space model. The main purpose of state space analysis is to infer the relevant properties of the \\(\\alpha_{t}\\)’s from a knowledge of the observations \\(y_{1},\\dots,y_{n}\\). The simple linear Gaussian state space model is given by\r\n\\[\r\n\\begin{aligned}\r\ny_{t} &=Z_{t} \\alpha_{t}+\\varepsilon_{t}, & & \\varepsilon_{t} \\sim \\mathrm{N}\\left(0, H_{t}\\right) \\\\\r\n\\alpha_{t+1} &=T_{t} \\alpha_{t}+R_{t} \\eta_{t}, & & \\eta_{t} \\sim \\mathrm{N}\\left(0, Q_{t}\\right), & t=1, \\ldots, n,\r\n\\end{aligned}\r\n\\] where \\(y_{t}\\) is a \\(p \\times 1\\) vector of observations called the observation vector and \\(\\alpha_{t}\\) is an unobserved \\(m \\times 1\\) vector called the state vector. The idea underlying the model is that the development of the system over time is determined by \\(\\alpha_{t}\\) according to the second equation of, but because \\(\\alpha_{t}\\) cannot be observed directly we must base the analysis on observations \\(y_{t}\\). The first equation of is called the observation equation and the second is called the state equation. The matrices \\(Z_{t}, T_{t}, R_{t}, H_{t} and Q_{t}\\) are initially assumed to be known and the error terms \\(\\epsilon_{t}\\) and \\(\\eta_{t}\\)t are assumed to be serially independent and independent of each other at all time points. In practice, some or all of the matrices \\(Z_{t}, T_{t}, R_{t}, H_{t} and Q_{t}\\) will depend on elements of an unknown parameter vector \\(psi\\) that needs to be estimated. Estimation of \\(\\psi\\) is generally performed by maximizing the prediction error decomposition form of the likelihood, that is\r\n\\[\r\n\\log L\\left(Y_{n}\\right)=-\\frac{n p}{2} \\log 2 \\pi-\\frac{1}{2} \\sum_{t=1}^{n}\\left(\\log \\left|F_{t}\\right|+v_{t}^{\\prime} F_{t}^{-1} v_{t}\\right).\r\n\\]\r\nIn order to obtain the prediction error decomposition of the likelihood the Kalman Filter is generally adopted. The Kalman Filter consists of the following set of recursive equations.\r\n\\[\r\n\\begin{aligned}\r\nv_{t} &=y_{t}-Z_{t} a_{t}, & F_{t} &=Z_{t} P_{t} Z_{t}^{\\prime}+H_{t} \\\\\r\na_{t \\mid t} &=a_{t}+P_{t} Z_{t}^{\\prime} F_{t}^{-1} v_{t}, & P_{t \\mid t} &=P_{t}-P_{t} Z_{t}^{\\prime} F_{t}^{-1} Z_{t} P_{t} \\\\\r\na_{t+1} &=T_{t} a_{t}+K_{t} v_{t}, & P_{t+1} &=T_{t} P_{t}\\left(T_{t}-K_{t} Z_{t}\\right)^{\\prime}+R_{t} Q_{t} R_{t}^{\\prime}\r\n\\end{aligned}\r\n\\]\r\nThe application of the Kalman Filter requires that the initial condition of the systems \\(\\alpha_{1} \\sim N(a_{1},P_{1})\\) are known. This rarely happens in practice and when the initial conditions of the system are not known a diffuse initialization is adopted. In practice diffuse initialization consists of initialize the distribution of \\(\\alpha_{1}\\) with a diffuse prior distribution. When the diffuse initialization is adopted the resulting likelihood is\r\n\\[\r\n\\log L_{d}\\left(Y_{n}\\right)=-\\frac{n p}{2} \\log 2 \\pi-\\frac{1}{2} \\sum_{t=1}^{d} w_{t}-\\frac{1}{2} \\sum_{t=d+1}^{n}\\left(\\log \\left|F_{t}\\right|+v_{t}^{\\prime} F_{t}^{-1} v_{t}\\right)\r\n\\]\r\nwhere \\(w_{t}\\) is a variable related to the diffuse initialization, see (Durbin and Koopman 2012) for a detail explanation of the diffuse initialization.\r\nFiltering, Smoothing and Forecasting\r\nOnce a model has been casted in state space it is then possible to perform three different types of “inference”:\r\nFiltering.In Filtering the rucursions of the Kalman Filter are used to recover the states distributions conditioned on the observed series \\(y_{1}, \\dots, y_{T}\\), that is, \\(f(\\alpha_{t}|y_{1:T})\\).\r\nSmoothing. In Smoothing the interest is to retrospectively reconstruct the behavior of the system. While in filter recursions are taken forward starting from \\(t = 1\\), in this case, we use a backward-recursive algorithm to compute the conditional distributions of \\(\\alpha_{t}\\) given \\(y_{1:T}\\) , for any \\(t < T\\), starting from the filtering distribution \\(f(\\alpha_{t} |y_{1:T} )\\) and estimating backward all the states history.\r\nForecasting With y1:t at hand, one can be interested in forecasting future values of the observations, \\(Y_{t+k}\\), or of the state vectors, \\(\\alpha_{t+k}\\). For state space models, the recursive form of the computations makes it natural to compute the one-step ahead forecasts and to update them sequentially as new data become available. The one-step ahead forecasts corresponds to \\(E(Y_{t+1}|y_{1:t})\\) for the observed series and to \\(E(\\alpha_{t+1}|\\alpha_{1:t})\\) for the states.\r\nRegression with time varying parameters\r\nState Space form represent a natural way to represent a linear regression with time varying parameters. In this case the regression equation represent the observation equation and the parameters are modeled as the unobserved states of the system. The observation equation is formulated as\r\n\\[ y_{t} = x'_{t}\\beta_{t} + \\epsilon_{t} \\quad \\epsilon_{t} \\sim N(0,\\sigma^{2})\\]\r\nwhere \\(y_{t}\\) is the response variable observed at time \\(t\\), \\(x_{t}\\) is a \\(k \\times 1\\) vector of regressors, \\(\\beta_{t}'\\) is a \\(k \\times 1\\) vector of time varying parameters and \\(\\epsilon_{t}\\) is the error term. In order to describe the behavior of the parameters the a state equation must be specified. If the parameters \\(\\beta_{t}\\) are assumed follow a random walk model the full specification of the DLM becomes\r\n\\[\r\n\\begin{aligned}\r\ny_{t} &= x'_{t} \\beta_{t}+\\varepsilon_{t} & & \\varepsilon_{t} \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right) \\\\\r\n\\beta_{t+1} &= \\beta_{t}+\\eta_{t} & & \\eta_{t} \\sim \\mathrm{N}\\left(0, Q \\right) & t=1, \\ldots, n,\r\n\\end{aligned}\r\n\\] where \\(T_{t} = I\\) and \\(Z_{t} = x'_{t}\\). The behavior of the parameters is guided by the matrix \\(Q\\), when \\(Q = 0\\) the model reduces the standard static linear regression model.The specification of the dynamic behavior of the parameters \\(\\beta_{t}\\) is generally guided by underlying assumptions or empirical evidence of the phenomena under study. For example when cyclical behavior is observed an autoregressive process of order 2, can be used to model the parameters.That is,\r\n\\[ \r\n\\begin{aligned}\r\ny_{t} &= x'_{t} \\beta_{t}+\\varepsilon_{t}  \\quad &\\varepsilon_{t} \\sim \\mathrm{N}\\left(0, \\sigma^{2}_{t}\\right) \\\\\r\n\\beta_{t+1} &= \\phi_{0} + \\phi_{1}\\beta_{t-1} + \\phi_{2} \\beta_{t-2} + \\eta_{t} \\quad &\\eta_{t} \\sim N(0,Q). \r\n\\end{aligned}\r\n\\]\r\nDynamic Marketing Mix Model\r\nData description\r\nThe data contains sales (expressed in thousand of units) and the corresponding advertising budget (expressed in thousand of dollars) for several media. The data ranges from 2001/01/01 to 2017/08/01 for a total of 200 observations.\r\nPreliminary Analysis\r\n\r\n\r\n\r\n\r\n\r\n\r\nA time series plot reveals that there is a great difference between the budget allocated to the different media. TV represents the media to which more money are allocated and the investments are divided depending on the TV program. Moreover, it emerges clearly that when some media are used other are stopped. In particular overt the period 2009-2011 no budget was allocated to Search and social media but a high investment was allocated to tv_cricket.\r\n\r\n\r\n\r\nFrom the correlation matrix it appears that sales are positively correlated with the budget allocated to TV, Radio, Social, Display rest and Search while there is no apparent linear relationship with the other media. TV and Radio represent the classical media more correlated with sales, while Social, Display_Rest and Seaarch are the new media with the highest correlation. As it is evident from the correlation matrix there is a strong positive correlation between the budget allocated to different media.For example there is an high positive correlation between Social, tv_sponsorship, Display_Rest and Search, meaning that when the budget in increased in one of these media is increased also in the others. We proceed to specify the media mix model.\r\nMedia mix model specification\r\nA general model specification for media mix modeling is the standard linear regression model,\r\n\\[ y_{t} = \\mu_{t} + \\gamma_{t} + x_{t}'\\beta + \\epsilon_{t} \\quad \\epsilon_{t} \\sim N(0,\\sigma^{2}) \\]\r\nwhere \\(\\mu_{t}\\) is the trend component representing the long run beahvior of the series, \\(\\gamma_{t}\\) is the seasonal component, \\(x'_{t}\\) is the vector of covariates, \\(\\beta\\) is the vector of unknown parameter and \\(\\epsilon_{t}\\) is the error term. When no seasonal behavior or trend are detected or assumed in the series the model reduces to\r\n\\[ y_{t}  = x'_{t}\\beta + \\epsilon_{t} \\quad \\epsilon_{t} \\sim N(0,\\sigma^{2}).\\]\r\nAs stated in the introduction one limitation of the plain linear regression model is that the marginal effects of the explanatory variables, represented by \\(\\beta\\), remains constant over time. Nevertheless, in some context, it might be more natural to allow the vector \\(\\beta\\) to evolve with time. In this example we will consider the following model\r\n\\[\r\n\\begin{aligned}\r\ny_{t} &= x_{t}'\\beta_{t} + \\epsilon_{t} \\quad &\\epsilon_{t} \\sim N(0,\\sigma^{2}) \\\\\r\n\\beta_{t+1} &= \\phi_{0} + \\phi_{1}\\beta_{t-1} + \\eta_{t}  \\quad &\\eta_{t} \\sim N(0, Q)\r\n\\end{aligned}\r\n\\]\r\nwhere the behavior of the parameters is modeled as a random walk process.\r\nFittinig dynamic models in R: the dlm package\r\nThe package that we will use to fit the dynamic linear model is the dlm package, developed by Giovanni Petris, which provides a flexible environment for fitting Dynamic Linear Gaussian models. Before to be fitted the data needs to be transformed into a ts object. Since we are working with monthly data we set the frequency equal to 12.\r\n\r\n\r\n# convert the data to a ts object\r\nadv.ts <- ts(advertising, start = advertising$Time[1], frequency = 12)\r\n\r\n\r\n\r\nNext, we extract the response variable (sales) and the explanatory variables, and proceed to fit the model and obtain the filtered and the smoothed states.\r\n\r\n\r\nx <-  adv.ts[,2:13]\r\ny <-  adv.ts[,14] \r\n\r\n# Define the linear regression model\r\n# Logarithmic transformation is applied to variance components to increase\r\n# the stability of the Kalman Filter\r\nbuildTVREG <-  function(parm, x.mat){\r\n  # parametrize the model as log(variance)\r\n  parm <- exp(parm)\r\n  # return function output\r\n  return(dlmModReg(X = x.mat, dV = parm[1], dW = parm[2:14]))\r\n}\r\n\r\n# Set initial values for optimizer\r\nstart.values = rep(0,14)\r\n\r\n# Obtain maximum likelihood estimates\r\nTVREG.mle = dlmMLE(y = y, parm = start.values, x.mat = x, buildTVREG)\r\n\r\n\r\n\r\nIn order to be sure that the optimizer converged we check the convergence criterion\r\n\r\n\r\nTVREG.mle$convergence\r\n\r\n\r\n[1] 0\r\n\r\nA zero values represent that the optimizer converged. The resulting estimated variances are\r\n\r\n [1] 1.190284e+00 7.784013e-01 1.509632e+02 5.487937e+01 4.844075e+00\r\n [6] 2.106967e+03 2.454546e+01 9.248649e-02 1.956561e-35 8.046326e+00\r\n[11] 5.258666e+02 3.476090e+00 9.725586e+02 1.729687e+00\r\n\r\nThe estimated variances can be built into the model to obtain the filtered and the smoothed states, which in our dynamic linear model correspond to the unknown time varying parameters.\r\n\r\n\r\n# Assign parameters to the estimated model\r\nTVREG.dlm <- buildTVREG(TVREG.mle$par,x)\r\n\r\n# Obtain fitlered states\r\nTVREG.f <- dlmFilter(y,TVREG.dlm)\r\n\r\n# Obtain smoothed states\r\nTVREG.s <-  dlmSmooth(TVREG.f)\r\n\r\n\r\n\r\nOnce the smoothed states are calculated we proceed to plot them over the period considered.\r\n\r\n\r\n# Extract smoothed states\r\ns = TVREG.s$s\r\n\r\nTime = advertising$Time\r\n# Assign column names to the smoothed states\r\ncolnames(s) <- paste(\"s\", sep = \"_\",c(\"Intercept\",colnames(x)))\r\n\r\n# Plot the smoothed states\r\ns  %>% as_tibble() %>% \r\n  filter(between(row_number(),1,n()-1)) %>% \r\n  mutate(Time = Time) %>% \r\n  pivot_longer(cols = -c(\"Time\"), names_to = \"States\") %>% \r\n  ggplot(aes (x = Time, y = value, color = States)) +\r\n  geom_line() +\r\n  ylab(\"Smoothed State\") +\r\n  xlab(\"\")+\r\n  facet_wrap(.~States, scales = \"free\") +\r\n scale_x_date(date_breaks = \"1 year\",date_labels = \"%Y\") +\r\n  theme(strip.background = element_blank(),\r\n        strip.text.x = element_text(size = 7),\r\n  axis.text.x = element_blank(),\r\n  axis.ticks.x = element_blank()) \r\n\r\n\r\n\r\n\r\nFigure 1: Smoothed states\r\n\r\n\r\n\r\nFigure 1 shows all the smoothed states over the period considered. The smoothed intercept represent the expected volume of sales when no budget is allocated to any media while the other states represent the unitary increase in sales when another dollar is allocated to the corresponding media. It is apparent that the variability in the smoothed states is different among the different media. The intercept shows an upward trend over the entire period, however the size of the increment is particularly small and to refine the model a fixed parameter restriction seems reasonable. The marginal effect of Magazines and Native is negative for the entire period while those of Search and Programmatic alternates from negative to positive values. Overall tv_cricket, tv_sponsorships, Radio and Social appear the safest media to which allocate budget. Overall despite new media represent good opportunities, traditional media appear to be more reliable form of investment. It is important to note that despite NPP is only slightly correlated to sales the corresponding estimated parameter shows a steady positive trend.\r\nElasticities\r\nOnce the model is estimated we can use it to calculate the elasticity of sales to the different media. The price elasticity (PED) is a measure of the responsiveness of the quantity demanded of a good to a change in its price When PED is greater than one, demand is elastic. This can be interpreted as consumers being very sensitive to changes with respect to a given variable \\(x\\): a \\(1\\%\\) increase in \\(x\\) will lead to a drop in quantity demanded of more than \\(1\\%\\). When PED is less than one, demand is inelastic. This can be interpreted as consumers being insensitive to changes in \\(x\\): a \\(1\\%\\) increase in price will lead to a drop in quantity demanded of less than \\(1\\%\\). Given that the estimated model is\r\n\\[ \r\n\\widehat{sales}_{t}  = \\hat{\\beta}_{0t} +  \\sum_{j=1}^{k} x_{jt} \\times \\hat{\\beta}_{jt} \r\n\\]\r\nwhere each \\(\\beta_{t}\\) is modeled as a random walk process, the elasticity of sales with respect to a media can be obtained as\r\n\\[\r\nE_{jt} = \\frac{\\Delta sales}{\\Delta x_{j}} \\times \\frac{x_{j}}{sales} = \\beta_{jt} \\frac{\\overline{x}_{j}}{\\overline{sales}}.\r\n\\]\r\nwhere \\(\\overline{x}_{j}\\) and \\(\\overline{sales}\\) represent the mean of the \\(j\\)th explanatory variable and of the sales. Another common approach to estimate elasticity with regression models is to adopt a double log transformation, which result in a model of the form\r\n\\[\r\nlog(y_{t}) = log(x'_{t})\\beta + \\epsilon_{t}.\r\n\\] Under this transformation the estimated parameter \\(\\hat{\\beta}\\) correspond to the value of the elasticity.\r\nWe proceed to compute the elasticity of sales with the media that appeared more correlated to them.\r\n\r\n\r\n# Obtain the elasticities from the smoothed states\r\nelasticities <- s %>% as_tibble() %>%\r\n  filter(between(row_number(),1,n()-1)) %>% \r\n  cbind(advertising) %>% \r\n  # calculate elasticities\r\n  mutate(tv_cricket_el = s_tv_cricket*(mean(tv_cricket)/mean(sales)),\r\n         tv_RON_el = s_tv_RON*(mean(tv_RON)/mean(sales)),\r\n         tv_sponsor_el = s_tv_sponsorships*(mean(tv_sponsorships)/mean(sales)),\r\n         Radio_el = s_radio*(mean(radio)/mean(sales)),\r\n         Social_el = s_Social*(mean(Social)/mean(sales)),\r\n         Display_el = s_Display_Rest*(mean(Display_Rest)/mean(sales)),\r\n         Search_el = s_Search*(mean(Search)/mean(sales))) %>%\r\n  # select time and elasticities\r\n  select(Time,contains(\"el\")) \r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 2: Estimated elasticities\r\n\r\n\r\n\r\nIt appears that sales are inelastic to all the media shown in figure 2.This means that the sales increase less than proportionally to the investment of the corresponding media. In other words a \\(1\\%\\) increase in investment in of the media considered produces an increase of less than \\(1\\%\\) in sales. The sales elasticity to Display shows an increasing trend while the sales elasticity to tv_Ron, on the opposite, shows a negative trend that reaches 0. Overall the classical media: rv_sponsorship, tv_cricket and Radio are the media with the highest elasticity.\r\nFinally we can forecast the future states. In this example we forecast the states for the next 12 months.\r\n\r\n\r\n\r\ns_Intercept\r\n\r\n\r\ns_tv_sponsorships\r\n\r\n\r\ns_tv_cricket\r\n\r\n\r\ns_tv_RON\r\n\r\n\r\ns_radio\r\n\r\n\r\ns_NPP\r\n\r\n\r\ns_Magazines\r\n\r\n\r\ns_OOH\r\n\r\n\r\ns_Social\r\n\r\n\r\ns_Programmatic\r\n\r\n\r\ns_Display_Rest\r\n\r\n\r\ns_Search\r\n\r\n\r\ns_Native\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\nNaN\r\n\r\n\r\n\r\nThe predictions obtained modeling the states as random walks are not really informative as in this case the best forecast is simply equal to previous period value. The adoption of a different stochastic process such as a random walk with drift or an autoregressive process to model the state (in this case the parameters) dynamics could be more useful for this purpose.\r\nConclusions\r\nIn this example we showed how state space models can be used to augment the information provided by static time series models. The state space formulation is general and can be adopted for many different models. With it and the Kalman Filter, smoothing, filtering and predicting can be performed. In this particular example the analysis of the smoothed states in the Dynamic Linear Regression showed how the marginal effect of the different media changed over the period considered. It is important to note that the behavior of the estimated parameters might also be a symptom of misspecification. In fact, as more variable are added to the model or a different specification is suggested the smoothed states might result in constant values. Moreover, in this particular case the estimated variances resulted particularly small and the variation observed in the smoothed state could be negligible for practical purposes.\r\n\r\n\r\n\r\nChan, David, and Mike Perry. 2017. “Challenges and Opportunities in Media Mix Modeling.”\r\n\r\n\r\nDurbin, James, and Siem Jan Koopman. 2012. Time Series Analysis by State Space Methods. Oxford university press.\r\n\r\n\r\nHarvey, Andrew C, and Clara Fernandes. 1989. “Time Series Models for Count or Qualitative Observations.” Journal of Business & Economic Statistics 7 (4): 407–17.\r\n\r\n\r\nPetris, Giovanni, Sonia Petrone, and Patrizia Campagnoli. 2009. “Dynamic Linear Models.” In Dynamic Linear Models with R, 31–84. Springer.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-01-08T21:28:14+01:00",
    "input_file": {}
  }
]
